{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinhDinh-4869/GameAwardPrediction/blob/main/Game_Awards_Prediction_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EkYfC9tvUBJ",
        "outputId": "b38e25a5-c280-4d1b-dc0b-9552ef1bbea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "import os\n",
        "\n",
        "path = '/content/gdrive/My Drive/CompulsoryElectiveData/'\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "W67wVxPovvOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def computeMeanScore(data):\n",
        "  items = data.strip().split(' ')\n",
        "  items = [float(x.strip()) for x in items]\n",
        "  return float(\"{:.2f}\".format(np.mean(np.array(items))))\n",
        "\n",
        "def computeSumScore(data):\n",
        "  items = data.strip().split(' ')\n",
        "  items = [float(x.strip()) for x in items]\n",
        "  #return 1/(float(\"{:.2f}\".format(np.sum(np.array(items))))/100 + 1)\n",
        "  return 1/(float(\"{:.2f}\".format(np.sum(np.array(items))))/100 + 1)\n",
        "  #return sigmoid(float(\"{:.2f}\".format(np.sum(np.array(items)))))\n",
        "\n",
        "def computeStandardDev(data):\n",
        "  items = data.strip().split(' ')\n",
        "  items = [float(x.strip()) for x in items]\n",
        "  return 1/((np.std(np.array(items))) + 1)\n",
        "\n",
        "def getLabel(data):\n",
        "  result = sum([int(x) for x in data])\n",
        "  if result > 0:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def processData(filename):\n",
        "  filehandle = open(filename)\n",
        "  X_train = []\n",
        "  Y_train = []\n",
        "  for line in filehandle.readlines()[1:]:\n",
        "    item = line.strip().split(',')[-10:] #remove names and years\n",
        "    #datapoint = [computeMeanScore(item[0])/100, computeMeanScore(item[1])/10, computeSumScore(item[2])] + [float(x) for x in item[3:6]]\n",
        "    datapoint = [computeMeanScore(item[0])/100, computeMeanScore(item[1])/10, computeSumScore(item[2])] + [float(x) for x in item[3:6]]\n",
        "    #print(datapoint)\n",
        "    print(computeStandardDev(item[2]))\n",
        "    label = getLabel(item[6:10])#int(item[-1]) #6, 7, 8,9\n",
        "\n",
        "    X_train.append(datapoint)\n",
        "    Y_train.append(label)\n",
        "  return np.asarray(X_train, dtype='float32'),np.asarray(Y_train, dtype='float32')\n",
        "\n",
        "data, label = processData('GameAwardsDataset2.csv')"
      ],
      "metadata": {
        "id": "rXv-JvDUvybq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd22296-94a8-4cdd-917f-a0fdb55e8c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0005724363072372106\n",
            "0.001271304262772299\n",
            "0.0006174805437350455\n",
            "0.0010319917440660474\n",
            "1.0\n",
            "1.0\n",
            "0.0038897075808186072\n",
            "0.008217118310370954\n",
            "0.012474476573140367\n",
            "0.0006916186772747227\n",
            "0.0009545313476075231\n",
            "1.0\n",
            "0.001976224004290587\n",
            "1.0\n",
            "1.0\n",
            "0.008888888888888889\n",
            "1.0\n",
            "0.051572109322655046\n",
            "0.02665368790385462\n",
            "0.007064576812732522\n",
            "0.016755798684445402\n",
            "1.0\n",
            "0.03017944880127956\n",
            "1.0\n",
            "0.18795247918520225\n",
            "0.04286280954741517\n",
            "0.03389830508474576\n",
            "1.0\n",
            "1.0\n",
            "0.003062429984267013\n",
            "0.0011886421272357184\n",
            "0.06896551724137931\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.003952569169960474\n",
            "0.005128205128205128\n",
            "0.0006524916706168378\n",
            "1.0\n",
            "0.004952779383565121\n",
            "1.0\n",
            "1.0\n",
            "0.009950248756218905\n",
            "0.003189792663476874\n",
            "1.0\n",
            "1.0\n",
            "0.01694915254237288\n",
            "0.0010328242923951865\n",
            "0.0011055570594479484\n",
            "1.0\n",
            "0.0005867712600360535\n",
            "0.018518518518518517\n",
            "1.0\n",
            "0.05128205128205128\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.029850746268656716\n",
            "1.0\n",
            "0.0015487462688049834\n",
            "1.0\n",
            "0.002886002886002886\n",
            "1.0\n",
            "0.1\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.013629370909829927\n",
            "1.0\n",
            "1.0\n",
            "0.09090909090909091\n",
            "1.0\n",
            "0.0006902780543698893\n",
            "1.0\n",
            "0.007967001549339001\n",
            "0.0018499724965019492\n",
            "1.0\n",
            "1.0\n",
            "0.006723786495874313\n",
            "0.001753216031679459\n",
            "0.0025906735751295338\n",
            "1.0\n",
            "0.00641025641025641\n",
            "0.00641513699017158\n",
            "0.0036369267964163343\n",
            "0.003241195461269737\n",
            "0.0029228670968676875\n",
            "0.019478425501228927\n",
            "0.06451612903225806\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.004597701149425287\n",
            "0.004881850635414398\n",
            "0.01160948141772559\n",
            "0.03864030842920624\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0015820988639485224\n",
            "0.001840951438493084\n",
            "1.0\n",
            "0.004149377593360996\n",
            "0.03560059130415701\n",
            "0.007704178679545138\n",
            "0.05508759528731462\n",
            "0.003430531732418525\n",
            "0.013232785732371305\n",
            "1.0\n",
            "0.011504315785975652\n",
            "0.10171118008217982\n",
            "1.0\n",
            "0.0034662045060658577\n",
            "0.0001634703764762694\n",
            "0.0006188118811881188\n",
            "0.0002058672156459084\n",
            "1.0\n",
            "0.0008613264427217916\n",
            "1.0\n",
            "1.0\n",
            "0.0015551743734594562\n",
            "0.003781913069540976\n",
            "0.01106386670241656\n",
            "0.007352941176470588\n",
            "0.001119194180190263\n",
            "0.05555555555555555\n",
            "0.0017781989044837326\n",
            "0.09523809523809523\n",
            "0.043157297798367085\n",
            "0.05\n",
            "1.0\n",
            "0.08\n",
            "1.0\n",
            "1.0\n",
            "0.2857142857142857\n",
            "0.6666666666666666\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0049261083743842365\n",
            "1.0\n",
            "0.00403411345453334\n",
            "0.0031177715459531183\n",
            "1.0\n",
            "1.0\n",
            "0.0006637902422834385\n",
            "0.010580563736590033\n",
            "0.005514320034612863\n",
            "1.0\n",
            "0.01587527399746164\n",
            "0.3333333333333333\n",
            "0.0066697013048366\n",
            "1.0\n",
            "1.0\n",
            "0.006120824438625769\n",
            "1.0\n",
            "0.006600660066006601\n",
            "1.0\n",
            "0.25\n",
            "0.01662315791391674\n",
            "0.004\n",
            "0.007163583776759927\n",
            "0.004197522265658686\n",
            "0.011342647809903057\n",
            "0.028115819828422695\n",
            "0.002977994506472182\n",
            "0.021282065181343658\n",
            "1.0\n",
            "1.0\n",
            "0.04878048780487805\n",
            "0.01253254190356764\n",
            "0.004679443513645354\n",
            "1.0\n",
            "0.14721652420118875\n",
            "1.0\n",
            "1.0\n",
            "0.0072992700729927005\n",
            "0.4\n",
            "0.12021880592029692\n",
            "0.0002992220227408737\n",
            "0.0006951685783802572\n",
            "1.0\n",
            "0.008851711964343065\n",
            "1.0\n",
            "0.03331060782002793\n",
            "0.002665935997521244\n",
            "0.01078805127759976\n",
            "0.0973366881823024\n",
            "0.010362694300518135\n",
            "0.0042643923240938165\n",
            "0.08695652173913043\n",
            "0.007218832097529562\n",
            "1.0\n",
            "0.009288285157375243\n",
            "1.0\n",
            "0.0377988317198079\n",
            "1.0\n",
            "0.005319148936170213\n",
            "0.0034593349091809064\n",
            "0.06125167087452341\n",
            "0.1988988169400271\n",
            "0.2222222222222222\n",
            "1.0\n",
            "1.0\n",
            "0.01565501223165249\n",
            "0.004957058309432337\n",
            "0.028072486907270364\n",
            "0.009533612076394627\n",
            "0.00684931506849315\n",
            "1.0\n",
            "0.04183119679849645\n",
            "0.008264020151676698\n",
            "0.00975609756097561\n",
            "1.0\n",
            "0.02181007552347609\n",
            "1.0\n",
            "0.05968342812456525\n",
            "1.0\n",
            "1.0\n",
            "0.03426718101743952\n",
            "0.004763428964133175\n",
            "0.010069050693057423\n",
            "0.011478921182258118\n",
            "0.0016025641025641025\n",
            "1.0\n",
            "0.060261038136777065\n",
            "0.0528983272432972\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.07927594053203824\n",
            "0.013905673741999068\n",
            "1.0\n",
            "0.4\n",
            "1.0\n",
            "1.0\n",
            "0.07692307692307693\n",
            "0.0013473815103020217\n",
            "1.0\n",
            "1.0\n",
            "0.004012744353643527\n",
            "0.00034148389705621047\n",
            "0.007150451210011166\n",
            "1.0\n",
            "0.031223781707369423\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.005404558694387536\n",
            "0.0027156302418739117\n",
            "0.002538121152186766\n",
            "0.0006380894627023083\n",
            "0.025974025974025976\n",
            "1.0\n",
            "0.03893289869318258\n",
            "1.0\n",
            "0.0014684287812041115\n",
            "0.0011409013120365088\n",
            "1.0\n",
            "1.0\n",
            "0.05809276117275508\n",
            "1.0\n",
            "0.08333333333333333\n",
            "0.01536697662765579\n",
            "0.06666666666666667\n",
            "0.6666666666666666\n",
            "1.0\n",
            "0.08357996758657145\n",
            "1.0\n",
            "0.0008691716189096221\n",
            "0.018691588785046728\n",
            "0.006578947368421052\n",
            "0.011363636363636364\n",
            "0.011668505162115601\n",
            "0.016204189130703435\n",
            "0.0015911894764201412\n",
            "0.04878048780487805\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.03314550899279777\n",
            "1.0\n",
            "1.0\n",
            "0.022861260088376745\n",
            "0.022240951627348633\n",
            "0.0470137027774815\n",
            "0.012636145868911423\n",
            "0.07856442027221731\n",
            "0.0015974171683564035\n",
            "1.0\n",
            "0.0018461725828958215\n",
            "0.09090909090909091\n",
            "0.0072501060725579\n",
            "0.0572420687955619\n",
            "0.144260482891476\n",
            "0.06451612903225806\n",
            "0.07142857142857142\n",
            "1.0\n",
            "0.09063489594175526\n",
            "0.04875508560724108\n",
            "1.0\n",
            "0.03230054331682437\n",
            "1.0\n",
            "0.13246856263439125\n",
            "0.018518518518518517\n",
            "0.006920415224913495\n",
            "0.37041520828107966\n",
            "1.0\n",
            "1.0\n",
            "0.000959959580465273\n",
            "1.0\n",
            "1.0\n",
            "0.023529411764705882\n",
            "1.0\n",
            "1.0\n",
            "0.0017250088230295272\n",
            "0.001359559781997224\n",
            "0.03636363636363636\n",
            "0.05040704986227237\n",
            "0.04130355885982394\n",
            "0.014724387618126175\n",
            "0.0032733224222585926\n",
            "0.2222222222222222\n",
            "1.0\n",
            "0.012738395484155191\n",
            "0.002069863320672556\n",
            "0.0040650406504065045\n",
            "1.0\n",
            "0.011125550312383685\n",
            "0.035590927800447945\n",
            "0.23463706286929648\n",
            "0.03571428571428571\n",
            "0.023255813953488372\n",
            "0.004750593824228029\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.004549364351421294\n",
            "0.0055025942977036495\n",
            "1.0\n",
            "0.018141397125843405\n",
            "0.03056232363073633\n",
            "0.005595301470468883\n",
            "0.011108876926934769\n",
            "0.041248546311245096\n",
            "0.13333333333333333\n",
            "0.003465494877747946\n",
            "0.0020289320541460993\n",
            "1.0\n",
            "0.0017969568188903563\n",
            "1.0\n",
            "0.0012231624386328276\n",
            "1.0\n",
            "0.002107411733357439\n",
            "0.011841038581679385\n",
            "0.002503113406561974\n",
            "0.08488502388888851\n",
            "0.006038923546832367\n",
            "0.0006761335186311819\n",
            "1.0\n",
            "0.04426184391550604\n",
            "0.01376616805678502\n",
            "1.0\n",
            "0.015873015873015872\n",
            "0.0019256465183455195\n",
            "0.008459333176062074\n",
            "0.019859304844917523\n",
            "0.005018125892222055\n",
            "1.0\n",
            "0.013880148706278957\n",
            "1.0\n",
            "0.05441244915941119\n",
            "0.052307626988520824\n",
            "0.07574607911821095\n",
            "1.0\n",
            "1.0\n",
            "0.03588080122758503\n",
            "0.028310963979151258\n",
            "0.06308705286420688\n",
            "0.0020462001968702156\n",
            "0.012595024978043703\n",
            "0.0005691942272748917\n",
            "0.012833674387006696\n",
            "0.019512852849177656\n",
            "0.0070837904597112495\n",
            "0.035713340929866444\n",
            "0.0650000724154536\n",
            "0.006041223642443845\n",
            "0.017035107325798767\n",
            "0.0011465598060596232\n",
            "0.004768255822101637\n",
            "1.0\n",
            "0.013513513513513514\n",
            "0.004654063426821288\n",
            "0.07023358562337863\n",
            "0.045460012370872306\n",
            "0.02157207399601357\n",
            "0.038461538461538464\n",
            "0.01680672268907563\n",
            "1.0\n",
            "0.018644405927778527\n",
            "0.0025348542458808617\n",
            "0.009052527816430414\n",
            "1.983542126275617e-05\n",
            "1.0\n",
            "0.11978243827074593\n",
            "0.009068930198116174\n",
            "0.01184965438768266\n",
            "1.0\n",
            "0.006600660066006601\n",
            "0.014491209460137806\n",
            "1.0\n",
            "0.014084507042253521\n",
            "0.05\n",
            "1.0\n",
            "0.02075870769047956\n",
            "1.0\n",
            "1.0\n",
            "0.04647210602067426\n",
            "0.0017777777777777779\n",
            "1.0\n",
            "0.03736426478576777\n",
            "0.034596123346632736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bitwise_xor(x1, x2):\n",
        "  return (np.multiply(x1, x2) + np.multiply(1-x1, 1-x2))\n",
        "def validate(x_val, y_val, w):\n",
        "  # n 6 6 1 n 1\n",
        "  y_pred = sigmoid(x_val.dot(w.T))\n",
        "  y_pred = (y_pred > 0.7)\n",
        "  return np.sum(bitwise_xor(y_pred, y_val)) / np.shape(y_val)[0]\n",
        "\n",
        "def loss(y_true, y_pred):\n",
        "  return -1 * (y_true.T.dot(np.log(y_pred)) + (1 - y_true).T.dot(np.log(1 - y_pred)))\n",
        "\n",
        "def computeLoss(x_train, y_train, w):\n",
        "    y_pred = sigmoid(x_train.dot(w.T))\n",
        "    return -1 * (y_train.T.dot(np.log(y_pred)) + (1 - y_train).T.dot(np.log(1 - y_pred)))\n",
        "\n",
        "def logistics_regression(X_train, Y_train, X_val, Y_val, epochs=50, eta=0.001):\n",
        "  w_init = np.random.random((1, 6))\n",
        "  print(w_init)\n",
        "  W = [w_init]\n",
        "  for _ in range(epochs):\n",
        "    ids = np.random.permutation(X_train.shape[0])\n",
        "    for id in ids:\n",
        "      xi = X_train[id, :].reshape(1, 6)\n",
        "      yi = Y_train[id].reshape(1,1)\n",
        "\n",
        "      zi = xi.dot(W[-1].T) #1 x 6 6 x1\n",
        "      ai = sigmoid(zi[0]).reshape(1,1)\n",
        "\n",
        "      dw = (ai - yi).dot(xi) #1 1 1 6\n",
        "      w_new = W[-1] - eta*dw\n",
        "\n",
        "      print('epochs no.', _)\n",
        "      #print('loss: ', loss[-1])\n",
        "      print('accuracy: ',validate(X_val, Y_val, w_new), ' loss: ', computeLoss(X_train, Y_train, w_new))\n",
        "      if np.linalg.norm(W[-1]- w_new) < 1e-4:\n",
        "      #if loss[-1] < 0.01:\n",
        "        return W\n",
        "      print(np.linalg.norm(W[-1]- w_new))\n",
        "      W.append(w_new)\n",
        "  return W\n"
      ],
      "metadata": {
        "id": "Kdabk9IJSE0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP\n",
        "def mlp_predict(x, w1, w2, w3):\n",
        "  z1 = x.dot(w1.T) #1 x 6 x 6 x 10 --> 1 x 10\n",
        "  a1 = sigmoid(z1[0]).reshape(1, 10)\n",
        "\n",
        "  z2 = a1.dot(w2.T) #1 x 10 x 10 x 10 --> 1 x 10\n",
        "  a2 = sigmoid(z2[0]).reshape(1, 10)\n",
        "\n",
        "  z3 = a2.dot(w3.T) #1 x 10 x 10 x 1 --> 1 x 1\n",
        "  a3 = sigmoid(z3[0]).reshape(1, 1)\n",
        "  return a3\n",
        "\n",
        "def forward(X, w1, w2, w3):\n",
        "  N = X.shape[0]\n",
        "\n",
        "  z1 = X.dot(w1.T) #N x 6 x 6 x 10 --> N x 10\n",
        "  a1 = sigmoid(z1).reshape(N, 10)\n",
        "\n",
        "  z2 = a1.dot(w2.T) #N x 10 x 10 x 10 --> N x 10\n",
        "  a2 = sigmoid(z2).reshape(N, 10)\n",
        "\n",
        "  z3 = a2.dot(w3.T) #N x 10 x 10 x 1 --> N x 1\n",
        "  a3 = sigmoid(z3).reshape(N, 1)\n",
        "  return a3\n",
        "\n",
        "def mlp_validate(x_val, y_val, w1, w2, w3):\n",
        "  y_pred = forward(x_val, w1, w2, w3)\n",
        "  y_pred = (y_pred > 0.7)\n",
        "  return np.sum(bitwise_xor(y_pred, y_val)) / np.shape(y_val)[0]\n",
        "\n",
        "def mlp(X_train, Y_train, X_val, Y_val, epochs=50, eta=0.01):\n",
        "  #nonbiased\n",
        "  w0 = np.random.random((10, 6))\n",
        "  w1 = np.random.random((10,10))\n",
        "  w2 = np.random.random((1, 10))\n",
        "\n",
        "  W0 = [w0]\n",
        "  W1 = [w1]\n",
        "  W2 = [w2]\n",
        "  for _ in range(epochs):\n",
        "    idx = np.random.permutation(X_train.shape[0])\n",
        "    #stochastic gradient descent\n",
        "    for i in idx:\n",
        "      xi = X_train[i, :].reshape(1, 6)\n",
        "      yi = Y_train[i, :].reshape(1, 1)\n",
        "\n",
        "      z1 = xi.dot(W0[-1].T) #1 x 6 x 6 x 10 --> 1 x 10\n",
        "      a1 = sigmoid(z1[0]).reshape(1, 10)\n",
        "\n",
        "      z2 = a1.dot(W1[-1].T) #1 x 10 x 10 x 10 --> 1 x 10\n",
        "      a2 = sigmoid(z2[0]).reshape(1, 10)\n",
        "\n",
        "      z3 = a2.dot(W2[-1].T) #1 x 10 x 10 x 1 --> 1 x 1\n",
        "      a3 = sigmoid(z3[0]).reshape(1, 1)\n",
        "\n",
        "      #back propagation\n",
        "      e3 = a3 - yi\n",
        "      dw2 = e3.T.dot(a2)\n",
        "\n",
        "      e2 = np.multiply(e3.dot(W2[-1]), np.multiply(a2, 1 - a2))\n",
        "      dw1 = e2.T.dot(a1)\n",
        "\n",
        "      e1 = np.multiply(e2.dot(W1[-1]), np.multiply(a1, 1 - a1))\n",
        "      dw0 = e1.T.dot(xi)\n",
        "\n",
        "      w0_n = W0[-1] - eta*dw0\n",
        "      w1_n = W1[-1] - eta*dw1\n",
        "      w2_n = W2[-1] - eta*dw2\n",
        "\n",
        "      accu = mlp_validate(X_val, Y_val, w0_n, w1_n, w2_n)\n",
        "      print(_)\n",
        "      print('accuracy: ', accu)\n",
        "      if(np.linalg.norm(W0[-1] - w0_n) < 1e-4) and (np.linalg.norm(W1[-1] - w1_n) < 1e-4) and (np.linalg.norm(W2[-1] - w2_n) < 1e-4) and accu > 0.9:\n",
        "        return W0, W1, W2\n",
        "      print(np.linalg.norm(W0[-1] - w0_n))\n",
        "      print(np.linalg.norm(W1[-1] - w1_n))\n",
        "      print(np.linalg.norm(W2[-1] - w2_n))\n",
        "      print\n",
        "      W0.append(w0_n)\n",
        "      W1.append(w1_n)\n",
        "      W2.append(w2_n)\n",
        "  return W0, W1, W2\n"
      ],
      "metadata": {
        "id": "aMThndt1qxeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "indices = np.random.permutation(data.shape[0])\n",
        "\n",
        "X_train, Y_train = data[:400, :], label[:400]\n",
        "X_val, Y_val = data[400:, :], label[400:]\n",
        "\n",
        "#X_train, Y_train = data[indices[:350]], label[indices[:350]]\n",
        "#X_val, Y_val = data[indices[350:]], label[indices[350:]]\n",
        "\n",
        "\n",
        "#X_train, Y_train = data, label\n",
        "#X_val, Y_val = data, label\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 6)\n",
        "X_val = X_val.reshape(X_val.shape[0], 6)\n",
        "\n",
        "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
        "Y_val = Y_val.reshape(Y_val.shape[0], 1)\n",
        "\n",
        "Y_train1 = tf.keras.utils.to_categorical(Y_train, 2)\n",
        "Y_val1 = tf.keras.utils.to_categorical(Y_val, 2)\n",
        "\n",
        "\n",
        "print(np.sum(Y_val) / Y_val.shape[0])\n",
        "print(np.sum(Y_train) / Y_train.shape[0])"
      ],
      "metadata": {
        "id": "LLha1Q7lCimy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73217188-7724-4729-de2d-70ba04eb0173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.037037037037037035\n",
            "0.4425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MItmYsAix0mE",
        "outputId": "41cc06a8-a5a4-4bf9-d73d-be85ad674c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[141 212 229 104  73 211  94  70 305 139 119 291   5 393 134  35 373 366\n",
            " 303 372 371 296 331  92 233  62 158 163 407 414 380 295 171 111   2 190\n",
            " 311 121  38 149  36  53 337 223 109 424  21  28  88 181 283  51 254 377\n",
            "  66 416 318 180  97 156 324 154 315 217 348  68 178 263  34 173 282  30\n",
            " 131 252 349 157 269 338 347  29 390  86  95 257 234  71  23 292 409 322\n",
            " 185 164 118 413  72 219 280  96 363  84 130 192 317 101 267 389   4  10\n",
            " 253  15 354 300 388 287 247 415 335  25 299 183  69 228 369 114 399 350\n",
            "  83  93  81  58 140 187  90 231 361 275 100  39 186 419  85  82 124 218\n",
            " 421 344 397 408 395 261 307 325 357 147  19 203 284 206 309  55 329 193\n",
            " 319 398  31 244 298 410 243 159 273   3 323 351 394 105 202 245 204 246\n",
            " 406 215  50  77 151  74 239 208 224 199 195 418 321  42 230 120 358   0\n",
            " 264  46 116 207 201 392 102 135 248 339 221 281 152 383 145 336 404 148\n",
            " 385  56 258  20  26  76 184 138 401 128 379 146 136 286 387 272 166  27\n",
            "  47 386 289 242  91 214  61 426 374 382 112  60 249 405 260 182  11   1\n",
            " 266 213 241 274 341 333  37  18  32 368 237 153 313 137 346 304 312 370\n",
            " 226 106 302  16 225 268 200 133 367 232  75 122  80 294 290 265  64  52\n",
            " 240 197 115 278 332 279  45 110 417 155 165 160  59 170  87 375 262 420\n",
            " 271 191   7 326 127 270  54 259 176 209 425 177 144 251 172 277 161 117\n",
            " 108 123 396 343 285   6 352 250 316 378 308 142  79  98 175  22 205 340\n",
            " 301  40 356  63 306 222 227 310 293 365 174  24 362   8 403  99 107 129\n",
            "  43 391 126 384  13  78 210 132 411 334  41 220 381 314 194 320  14  49\n",
            "  48  57  89 345 402 353 216 113 189 143   9 360 297 168  33  17 198 422\n",
            " 196 276 328 376  12 179 423 236 150  65 359 125 355 256 238 364 400  44\n",
            " 412 162 330 327 169 288  67 103 255 235 167 342 188]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = logistics_regression(X_train, Y_train, X_val, Y_val)"
      ],
      "metadata": {
        "id": "4dQT_5EdWSkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1, W2, W3 = mlp(X_train, Y_train, X_val, Y_val)"
      ],
      "metadata": {
        "id": "zGkQSySttvTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#elden ring\n",
        "a = [computeMeanScore('96 94 90 96')/100, computeMeanScore('7.8 6.9 7 6.4 7.7')/10, computeSumScore('10646 4856 1031 469 2508')] + [1,1,5]\n",
        "a = np.asarray(a, dtype = 'float32')\n",
        "#a = X_train[323]\n",
        "#print(a)\n",
        "#stray\n",
        "b = [computeMeanScore('82 83 83')/100, computeMeanScore('8.4 8.3 8.5')/10, computeSumScore('508 163 1664')] + [0,0,3]\n",
        "b = np.asarray(b, dtype='float32')\n",
        "\n",
        "#gow : ragnarok\n",
        "c = [computeMeanScore('94 100')/100, computeMeanScore('8.0 6')/10, computeSumScore('5833 509')] + [1,1,2]\n",
        "c = np.asarray(c, dtype='float32')\n",
        "\n",
        "#one piece\n",
        "d = [computeMeanScore('35')/100, computeMeanScore('2.3')/10, computeSumScore('7')] + [0,0,1]\n",
        "d = np.asarray(d, dtype='float32')\n",
        "\n",
        "#martha is dead\n",
        "e = [computeMeanScore('73 65 72 58 65')/100, computeMeanScore('7.3 6.8 5.2 7 7')/10, computeSumScore('61 14 34 12 31')] + [0,0,5]\n",
        "e = np.asarray(d, dtype='float32')\n",
        "\n",
        "#Horizon forbidden quest\n",
        "f = [computeMeanScore('88 83')/100, computeMeanScore('8.8 7.2')/10, computeSumScore('9341 1049')] + [1,1,2]\n",
        "f = np.asarray(f, dtype='float32')\n",
        "\n",
        "#Tunic\n",
        "g = [computeMeanScore('85 86 88 85 85')/100, computeMeanScore('7.6 8 8.3 7.2 7.8')/10, computeSumScore('352 26 30 121 299')] + [0,0,6]\n",
        "g = np.asarray(g, dtype='float32')\n",
        "\n",
        "#A plague tale\n",
        "h = [computeMeanScore('82 82 76 85')/100, computeMeanScore('7.9 8.1 8.3 8.4')/10, computeSumScore('369 326 25 392')] + [1,1,4]\n",
        "h = np.asarray(h, dtype='float32')\n",
        "\n",
        "#xenoblade 3\n",
        "i = [computeMeanScore('89')/100, computeMeanScore('8.5')/10, computeSumScore('1146')] + [1,1,1]\n",
        "i = np.asarray(i, dtype='float32')\n",
        "\n",
        "#sonic frontier\n",
        "k = [computeMeanScore('75 81 71 69 61')/100, computeMeanScore('8.6 8.3 8.2 8.3 8.2 8.3')/10, computeSumScore('670 236 2716 404 219 360')] + [1,1,6]\n",
        "k = np.asarray(k, dtype='float32')\n",
        "\n",
        "#genshin impact\n",
        "j = [computeMeanScore('81 82 84 90')/100, computeMeanScore('4.6 3.2 4.1 2.1')/10, computeSumScore('1188 571 2817 357')] + [0,0,4]\n",
        "j = np.asarray(j, dtype='float32')\n",
        "\n",
        "#bayo 3\n",
        "l = [computeMeanScore('86')/100, computeMeanScore('7.2')/10, computeSumScore('522')] + [1,1,1]\n",
        "l = np.asarray(l, dtype='float32')\n",
        "\n",
        "a = a.reshape(1,6)\n",
        "b = b.reshape(1,6)\n",
        "c = c.reshape(1,6)\n",
        "d = d.reshape(1,6)\n",
        "e = e.reshape(1,6)\n",
        "f = f.reshape(1,6)\n",
        "g = g.reshape(1,6)\n",
        "h = h.reshape(1,6)\n",
        "i = i.reshape(1,6)\n",
        "k = k.reshape(1,6)\n",
        "j = j.reshape(1,6)\n",
        "l = l.reshape(1,6)"
      ],
      "metadata": {
        "id": "YVJHs6s7ab4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('myModel_softmax_2_3.h5')"
      ],
      "metadata": {
        "id": "4VyWqxometc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a0 = model.predict(a)\n",
        "b0 = model.predict(b)\n",
        "c0 = model.predict(c)\n",
        "d0 = model.predict(d)\n",
        "e0 = model.predict(e)\n",
        "f0 = model.predict(f)\n",
        "g0 = model.predict(g)\n",
        "h0 = model.predict(h)\n",
        "i0 = model.predict(i)\n",
        "k0 = model.predict(k)\n",
        "j0 = model.predict(j)\n",
        "l0 = model.predict(l)\n",
        "print(\"elden ring's winning award(s) probability: \",a0[0][1]) #elden ring\n",
        "print(\"stray's winning award(s) probability: \",b0[0][1]) #stray\n",
        "print(\"god of war's winning award(s) probability: \",c0[0][1]) #gow\n",
        "print(\"one piece grand cruise's winning award(s) probability: \",d0[0][1]) #one piece grand cruise\n",
        "print(\"martha is dead's winning award(s) probability: \",e0[0][1]) #martha is dead\n",
        "print(\"horizon's winning award(s) probability: \",f0[0][1]) #martha is dead\n",
        "print(\"tunic's winning award(s) probability: \",g0[0][1]) #martha is dead\n",
        "print(\"a plague tale's winning award(s) probability: \",h0[0][1]) #martha is dead\n",
        "print(\"xenoblade 3's winning award(s) probability: \",i0[0][1]) #martha is dead\n",
        "print(\"sonic frontier's winning award(s) probability: \",k0[0][1]) #martha is dead\n",
        "print(\"genshit's winning award(s) probability: \",j0[0][1]) #martha is dead\n",
        "print(\"bayonetta's winning award(s) probability: \",l0[0][1]) #martha is dead\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "atRN020PsSy6",
        "outputId": "a7ed18d9-6bc6-4470-f899-439716ef20c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a567498c86a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0md0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0me0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.compat.v1.losses.sigmoid_cross_entropy(y_true, y_pred, label_smoothing=0.1)\n",
        "\n",
        "def custom_loss_1(y_true, y_pred):\n",
        "  return -1 * (y_true.T.dot(np.log(y_pred)) + (1 - y_true).T.dot(np.log(1 - y_pred)))\n",
        "#init model\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(6, input_shape=(6,)))\n",
        "model.add(tf.keras.layers.Dense(10, activation = 'sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(10, activation = 'sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(10, activation = 'sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(2, activation = 'softmax'))\n",
        "\n",
        "#model.compile(optimizer = 'adam', loss = custom_loss, metrics = ['accuracy'])\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train1, epochs = 800, validation_data = (X_val, Y_val1))\n",
        "\n",
        "model.save(\"myModel_softmax_2_10.h5\")"
      ],
      "metadata": {
        "id": "5lV6lXVzAZB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b49730-8345-41de-cbdc-bd0ba61e5ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "13/13 [==============================] - 1s 19ms/step - loss: 0.6953 - accuracy: 0.5575 - val_loss: 0.5047 - val_accuracy: 0.9630\n",
            "Epoch 2/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.5575 - val_loss: 0.5609 - val_accuracy: 0.9630\n",
            "Epoch 3/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5575 - val_loss: 0.5934 - val_accuracy: 0.9630\n",
            "Epoch 4/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.5575 - val_loss: 0.5992 - val_accuracy: 0.9630\n",
            "Epoch 5/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5575 - val_loss: 0.5964 - val_accuracy: 0.9630\n",
            "Epoch 6/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5575 - val_loss: 0.6105 - val_accuracy: 0.9630\n",
            "Epoch 7/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6843 - accuracy: 0.5575 - val_loss: 0.6062 - val_accuracy: 0.9630\n",
            "Epoch 8/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6838 - accuracy: 0.5575 - val_loss: 0.5983 - val_accuracy: 0.9630\n",
            "Epoch 9/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6836 - accuracy: 0.5575 - val_loss: 0.5953 - val_accuracy: 0.9630\n",
            "Epoch 10/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5575 - val_loss: 0.5848 - val_accuracy: 0.9630\n",
            "Epoch 11/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6826 - accuracy: 0.5575 - val_loss: 0.5977 - val_accuracy: 0.9630\n",
            "Epoch 12/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6821 - accuracy: 0.5575 - val_loss: 0.5938 - val_accuracy: 0.9630\n",
            "Epoch 13/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6814 - accuracy: 0.5575 - val_loss: 0.5977 - val_accuracy: 0.9630\n",
            "Epoch 14/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5575 - val_loss: 0.5975 - val_accuracy: 0.9630\n",
            "Epoch 15/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6809 - accuracy: 0.5575 - val_loss: 0.5799 - val_accuracy: 0.9630\n",
            "Epoch 16/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6788 - accuracy: 0.5575 - val_loss: 0.5924 - val_accuracy: 0.9630\n",
            "Epoch 17/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5575 - val_loss: 0.6127 - val_accuracy: 0.9630\n",
            "Epoch 18/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6782 - accuracy: 0.5575 - val_loss: 0.6007 - val_accuracy: 0.9630\n",
            "Epoch 19/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6767 - accuracy: 0.5575 - val_loss: 0.6015 - val_accuracy: 0.9630\n",
            "Epoch 20/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6752 - accuracy: 0.5575 - val_loss: 0.6052 - val_accuracy: 0.9630\n",
            "Epoch 21/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6748 - accuracy: 0.5575 - val_loss: 0.5896 - val_accuracy: 0.9630\n",
            "Epoch 22/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6730 - accuracy: 0.5575 - val_loss: 0.5991 - val_accuracy: 0.9630\n",
            "Epoch 23/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6711 - accuracy: 0.5575 - val_loss: 0.5948 - val_accuracy: 0.9630\n",
            "Epoch 24/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6697 - accuracy: 0.5575 - val_loss: 0.5885 - val_accuracy: 0.9630\n",
            "Epoch 25/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6685 - accuracy: 0.5575 - val_loss: 0.5795 - val_accuracy: 0.9630\n",
            "Epoch 26/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5575 - val_loss: 0.5956 - val_accuracy: 0.9630\n",
            "Epoch 27/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5575 - val_loss: 0.6041 - val_accuracy: 0.9630\n",
            "Epoch 28/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5575 - val_loss: 0.5955 - val_accuracy: 0.9630\n",
            "Epoch 29/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6605 - accuracy: 0.5850 - val_loss: 0.6085 - val_accuracy: 0.9630\n",
            "Epoch 30/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.5700 - val_loss: 0.5829 - val_accuracy: 0.9630\n",
            "Epoch 31/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6558 - accuracy: 0.6050 - val_loss: 0.6082 - val_accuracy: 0.9259\n",
            "Epoch 32/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.6400 - val_loss: 0.6226 - val_accuracy: 0.8889\n",
            "Epoch 33/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6505 - accuracy: 0.6725 - val_loss: 0.5958 - val_accuracy: 0.9259\n",
            "Epoch 34/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6491 - accuracy: 0.6325 - val_loss: 0.5761 - val_accuracy: 0.9630\n",
            "Epoch 35/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6459 - accuracy: 0.6550 - val_loss: 0.5810 - val_accuracy: 0.9259\n",
            "Epoch 36/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6431 - accuracy: 0.6600 - val_loss: 0.5945 - val_accuracy: 0.8889\n",
            "Epoch 37/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6401 - accuracy: 0.6750 - val_loss: 0.6099 - val_accuracy: 0.8519\n",
            "Epoch 38/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6371 - accuracy: 0.6725 - val_loss: 0.5981 - val_accuracy: 0.8519\n",
            "Epoch 39/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6345 - accuracy: 0.6675 - val_loss: 0.5887 - val_accuracy: 0.8519\n",
            "Epoch 40/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6323 - accuracy: 0.6725 - val_loss: 0.6083 - val_accuracy: 0.8519\n",
            "Epoch 41/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6284 - accuracy: 0.6700 - val_loss: 0.5810 - val_accuracy: 0.8519\n",
            "Epoch 42/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6257 - accuracy: 0.6600 - val_loss: 0.5713 - val_accuracy: 0.8519\n",
            "Epoch 43/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6222 - accuracy: 0.6825 - val_loss: 0.6008 - val_accuracy: 0.8148\n",
            "Epoch 44/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6199 - accuracy: 0.6850 - val_loss: 0.5877 - val_accuracy: 0.8148\n",
            "Epoch 45/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.6850 - val_loss: 0.5912 - val_accuracy: 0.7407\n",
            "Epoch 46/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6146 - accuracy: 0.6825 - val_loss: 0.5892 - val_accuracy: 0.7407\n",
            "Epoch 47/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6098 - accuracy: 0.6875 - val_loss: 0.5709 - val_accuracy: 0.7407\n",
            "Epoch 48/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6073 - accuracy: 0.6925 - val_loss: 0.5716 - val_accuracy: 0.7407\n",
            "Epoch 49/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6045 - accuracy: 0.6975 - val_loss: 0.5679 - val_accuracy: 0.7407\n",
            "Epoch 50/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.7100 - val_loss: 0.5422 - val_accuracy: 0.7778\n",
            "Epoch 51/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.7000 - val_loss: 0.5512 - val_accuracy: 0.7407\n",
            "Epoch 52/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5956 - accuracy: 0.7050 - val_loss: 0.5540 - val_accuracy: 0.7407\n",
            "Epoch 53/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5931 - accuracy: 0.7125 - val_loss: 0.5406 - val_accuracy: 0.7407\n",
            "Epoch 54/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5905 - accuracy: 0.7050 - val_loss: 0.5525 - val_accuracy: 0.6667\n",
            "Epoch 55/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5884 - accuracy: 0.7050 - val_loss: 0.5421 - val_accuracy: 0.7037\n",
            "Epoch 56/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5865 - accuracy: 0.7225 - val_loss: 0.5118 - val_accuracy: 0.7407\n",
            "Epoch 57/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5827 - accuracy: 0.7225 - val_loss: 0.5423 - val_accuracy: 0.6667\n",
            "Epoch 58/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5808 - accuracy: 0.7000 - val_loss: 0.5467 - val_accuracy: 0.6667\n",
            "Epoch 59/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5782 - accuracy: 0.7100 - val_loss: 0.5151 - val_accuracy: 0.7037\n",
            "Epoch 60/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5772 - accuracy: 0.7250 - val_loss: 0.5113 - val_accuracy: 0.6667\n",
            "Epoch 61/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5746 - accuracy: 0.7075 - val_loss: 0.5244 - val_accuracy: 0.6667\n",
            "Epoch 62/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5735 - accuracy: 0.7150 - val_loss: 0.5121 - val_accuracy: 0.6667\n",
            "Epoch 63/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5716 - accuracy: 0.7200 - val_loss: 0.5163 - val_accuracy: 0.6667\n",
            "Epoch 64/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5706 - accuracy: 0.7175 - val_loss: 0.5482 - val_accuracy: 0.5926\n",
            "Epoch 65/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5682 - accuracy: 0.7125 - val_loss: 0.5097 - val_accuracy: 0.6667\n",
            "Epoch 66/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5669 - accuracy: 0.7100 - val_loss: 0.5022 - val_accuracy: 0.6667\n",
            "Epoch 67/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5662 - accuracy: 0.7150 - val_loss: 0.5014 - val_accuracy: 0.6667\n",
            "Epoch 68/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5655 - accuracy: 0.7175 - val_loss: 0.4974 - val_accuracy: 0.6667\n",
            "Epoch 69/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5643 - accuracy: 0.7200 - val_loss: 0.5101 - val_accuracy: 0.6667\n",
            "Epoch 70/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5667 - accuracy: 0.7150 - val_loss: 0.5415 - val_accuracy: 0.6296\n",
            "Epoch 71/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5616 - accuracy: 0.7250 - val_loss: 0.4839 - val_accuracy: 0.6667\n",
            "Epoch 72/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5623 - accuracy: 0.7350 - val_loss: 0.4768 - val_accuracy: 0.6667\n",
            "Epoch 73/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5612 - accuracy: 0.7250 - val_loss: 0.5018 - val_accuracy: 0.6667\n",
            "Epoch 74/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5605 - accuracy: 0.7250 - val_loss: 0.5101 - val_accuracy: 0.6667\n",
            "Epoch 75/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7150 - val_loss: 0.5084 - val_accuracy: 0.6667\n",
            "Epoch 76/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7250 - val_loss: 0.4881 - val_accuracy: 0.6667\n",
            "Epoch 77/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7200 - val_loss: 0.5175 - val_accuracy: 0.6667\n",
            "Epoch 78/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5597 - accuracy: 0.7100 - val_loss: 0.4952 - val_accuracy: 0.6667\n",
            "Epoch 79/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5577 - accuracy: 0.7250 - val_loss: 0.4989 - val_accuracy: 0.6667\n",
            "Epoch 80/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5575 - accuracy: 0.7250 - val_loss: 0.4976 - val_accuracy: 0.6667\n",
            "Epoch 81/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5572 - accuracy: 0.7275 - val_loss: 0.4811 - val_accuracy: 0.6667\n",
            "Epoch 82/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5567 - accuracy: 0.7250 - val_loss: 0.5026 - val_accuracy: 0.6667\n",
            "Epoch 83/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5571 - accuracy: 0.7225 - val_loss: 0.5025 - val_accuracy: 0.6667\n",
            "Epoch 84/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5563 - accuracy: 0.7325 - val_loss: 0.4676 - val_accuracy: 0.7037\n",
            "Epoch 85/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5556 - accuracy: 0.7325 - val_loss: 0.4990 - val_accuracy: 0.6296\n",
            "Epoch 86/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5562 - accuracy: 0.7200 - val_loss: 0.4843 - val_accuracy: 0.6667\n",
            "Epoch 87/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5547 - accuracy: 0.7200 - val_loss: 0.5013 - val_accuracy: 0.6296\n",
            "Epoch 88/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5558 - accuracy: 0.7200 - val_loss: 0.4731 - val_accuracy: 0.6667\n",
            "Epoch 89/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5546 - accuracy: 0.7250 - val_loss: 0.4897 - val_accuracy: 0.6667\n",
            "Epoch 90/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5547 - accuracy: 0.7200 - val_loss: 0.4883 - val_accuracy: 0.6667\n",
            "Epoch 91/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7200 - val_loss: 0.4682 - val_accuracy: 0.7037\n",
            "Epoch 92/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5537 - accuracy: 0.7200 - val_loss: 0.4792 - val_accuracy: 0.6667\n",
            "Epoch 93/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7225 - val_loss: 0.4770 - val_accuracy: 0.6667\n",
            "Epoch 94/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7225 - val_loss: 0.4763 - val_accuracy: 0.6667\n",
            "Epoch 95/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5535 - accuracy: 0.7225 - val_loss: 0.4673 - val_accuracy: 0.7037\n",
            "Epoch 96/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5529 - accuracy: 0.7175 - val_loss: 0.4801 - val_accuracy: 0.6667\n",
            "Epoch 97/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5536 - accuracy: 0.7225 - val_loss: 0.4827 - val_accuracy: 0.6296\n",
            "Epoch 98/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5535 - accuracy: 0.7175 - val_loss: 0.4970 - val_accuracy: 0.6296\n",
            "Epoch 99/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5513 - accuracy: 0.7175 - val_loss: 0.4449 - val_accuracy: 0.7407\n",
            "Epoch 100/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5520 - accuracy: 0.7300 - val_loss: 0.4596 - val_accuracy: 0.7037\n",
            "Epoch 101/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5517 - accuracy: 0.7250 - val_loss: 0.4791 - val_accuracy: 0.6296\n",
            "Epoch 102/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5520 - accuracy: 0.7250 - val_loss: 0.4528 - val_accuracy: 0.7037\n",
            "Epoch 103/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5515 - accuracy: 0.7175 - val_loss: 0.4812 - val_accuracy: 0.6296\n",
            "Epoch 104/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5510 - accuracy: 0.7200 - val_loss: 0.4605 - val_accuracy: 0.7037\n",
            "Epoch 105/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5504 - accuracy: 0.7275 - val_loss: 0.4504 - val_accuracy: 0.7407\n",
            "Epoch 106/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5506 - accuracy: 0.7250 - val_loss: 0.4633 - val_accuracy: 0.7037\n",
            "Epoch 107/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5506 - accuracy: 0.7275 - val_loss: 0.4530 - val_accuracy: 0.7037\n",
            "Epoch 108/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5500 - accuracy: 0.7300 - val_loss: 0.4590 - val_accuracy: 0.7037\n",
            "Epoch 109/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5520 - accuracy: 0.7175 - val_loss: 0.4742 - val_accuracy: 0.6667\n",
            "Epoch 110/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5502 - accuracy: 0.7225 - val_loss: 0.4422 - val_accuracy: 0.7407\n",
            "Epoch 111/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5496 - accuracy: 0.7300 - val_loss: 0.4589 - val_accuracy: 0.7037\n",
            "Epoch 112/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5499 - accuracy: 0.7300 - val_loss: 0.4561 - val_accuracy: 0.7037\n",
            "Epoch 113/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5496 - accuracy: 0.7250 - val_loss: 0.4379 - val_accuracy: 0.7407\n",
            "Epoch 114/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5505 - accuracy: 0.7275 - val_loss: 0.4388 - val_accuracy: 0.7407\n",
            "Epoch 115/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5499 - accuracy: 0.7200 - val_loss: 0.4695 - val_accuracy: 0.6667\n",
            "Epoch 116/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5489 - accuracy: 0.7225 - val_loss: 0.4651 - val_accuracy: 0.6667\n",
            "Epoch 117/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5494 - accuracy: 0.7250 - val_loss: 0.4404 - val_accuracy: 0.7407\n",
            "Epoch 118/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5483 - accuracy: 0.7275 - val_loss: 0.4540 - val_accuracy: 0.7037\n",
            "Epoch 119/800\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.5485 - accuracy: 0.7225 - val_loss: 0.4592 - val_accuracy: 0.7037\n",
            "Epoch 120/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5482 - accuracy: 0.7225 - val_loss: 0.4631 - val_accuracy: 0.6667\n",
            "Epoch 121/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.7250 - val_loss: 0.4475 - val_accuracy: 0.7407\n",
            "Epoch 122/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.7275 - val_loss: 0.4342 - val_accuracy: 0.7407\n",
            "Epoch 123/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5473 - accuracy: 0.7250 - val_loss: 0.4527 - val_accuracy: 0.7037\n",
            "Epoch 124/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5479 - accuracy: 0.7200 - val_loss: 0.4571 - val_accuracy: 0.6667\n",
            "Epoch 125/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5476 - accuracy: 0.7375 - val_loss: 0.4332 - val_accuracy: 0.7407\n",
            "Epoch 126/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5480 - accuracy: 0.7325 - val_loss: 0.4489 - val_accuracy: 0.7407\n",
            "Epoch 127/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5461 - accuracy: 0.7350 - val_loss: 0.4231 - val_accuracy: 0.7778\n",
            "Epoch 128/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5475 - accuracy: 0.7275 - val_loss: 0.4419 - val_accuracy: 0.7407\n",
            "Epoch 129/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5466 - accuracy: 0.7350 - val_loss: 0.4373 - val_accuracy: 0.7407\n",
            "Epoch 130/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5467 - accuracy: 0.7425 - val_loss: 0.4422 - val_accuracy: 0.7407\n",
            "Epoch 131/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5459 - accuracy: 0.7375 - val_loss: 0.4387 - val_accuracy: 0.7407\n",
            "Epoch 132/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5504 - accuracy: 0.7325 - val_loss: 0.4128 - val_accuracy: 0.7778\n",
            "Epoch 133/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5463 - accuracy: 0.7325 - val_loss: 0.4685 - val_accuracy: 0.6667\n",
            "Epoch 134/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5462 - accuracy: 0.7375 - val_loss: 0.4330 - val_accuracy: 0.7407\n",
            "Epoch 135/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5463 - accuracy: 0.7375 - val_loss: 0.4248 - val_accuracy: 0.7778\n",
            "Epoch 136/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5487 - accuracy: 0.7225 - val_loss: 0.4465 - val_accuracy: 0.7037\n",
            "Epoch 137/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5465 - accuracy: 0.7400 - val_loss: 0.4399 - val_accuracy: 0.7407\n",
            "Epoch 138/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5461 - accuracy: 0.7425 - val_loss: 0.4267 - val_accuracy: 0.7407\n",
            "Epoch 139/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5449 - accuracy: 0.7450 - val_loss: 0.4298 - val_accuracy: 0.7407\n",
            "Epoch 140/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5453 - accuracy: 0.7300 - val_loss: 0.4393 - val_accuracy: 0.7407\n",
            "Epoch 141/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5453 - accuracy: 0.7275 - val_loss: 0.4385 - val_accuracy: 0.7407\n",
            "Epoch 142/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5448 - accuracy: 0.7400 - val_loss: 0.4071 - val_accuracy: 0.7778\n",
            "Epoch 143/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5466 - accuracy: 0.7325 - val_loss: 0.4415 - val_accuracy: 0.7407\n",
            "Epoch 144/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5451 - accuracy: 0.7450 - val_loss: 0.4145 - val_accuracy: 0.7778\n",
            "Epoch 145/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5450 - accuracy: 0.7350 - val_loss: 0.4363 - val_accuracy: 0.7407\n",
            "Epoch 146/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.7425 - val_loss: 0.4294 - val_accuracy: 0.7407\n",
            "Epoch 147/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5436 - accuracy: 0.7350 - val_loss: 0.4025 - val_accuracy: 0.7778\n",
            "Epoch 148/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5439 - accuracy: 0.7375 - val_loss: 0.4221 - val_accuracy: 0.7778\n",
            "Epoch 149/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5441 - accuracy: 0.7450 - val_loss: 0.4300 - val_accuracy: 0.7407\n",
            "Epoch 150/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5438 - accuracy: 0.7350 - val_loss: 0.4357 - val_accuracy: 0.7407\n",
            "Epoch 151/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.7350 - val_loss: 0.4217 - val_accuracy: 0.7778\n",
            "Epoch 152/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5442 - accuracy: 0.7400 - val_loss: 0.4268 - val_accuracy: 0.7407\n",
            "Epoch 153/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5449 - accuracy: 0.7400 - val_loss: 0.4038 - val_accuracy: 0.7778\n",
            "Epoch 154/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5434 - accuracy: 0.7350 - val_loss: 0.4337 - val_accuracy: 0.7407\n",
            "Epoch 155/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5435 - accuracy: 0.7400 - val_loss: 0.4321 - val_accuracy: 0.7037\n",
            "Epoch 156/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5431 - accuracy: 0.7450 - val_loss: 0.4166 - val_accuracy: 0.7778\n",
            "Epoch 157/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5426 - accuracy: 0.7450 - val_loss: 0.4198 - val_accuracy: 0.7778\n",
            "Epoch 158/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5425 - accuracy: 0.7450 - val_loss: 0.4190 - val_accuracy: 0.7778\n",
            "Epoch 159/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5427 - accuracy: 0.7400 - val_loss: 0.4341 - val_accuracy: 0.7037\n",
            "Epoch 160/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5427 - accuracy: 0.7425 - val_loss: 0.4010 - val_accuracy: 0.7778\n",
            "Epoch 161/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5434 - accuracy: 0.7375 - val_loss: 0.4249 - val_accuracy: 0.7778\n",
            "Epoch 162/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.7425 - val_loss: 0.4160 - val_accuracy: 0.7778\n",
            "Epoch 163/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5437 - accuracy: 0.7475 - val_loss: 0.4199 - val_accuracy: 0.7778\n",
            "Epoch 164/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5413 - accuracy: 0.7475 - val_loss: 0.4088 - val_accuracy: 0.7778\n",
            "Epoch 165/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.7425 - val_loss: 0.4167 - val_accuracy: 0.7778\n",
            "Epoch 166/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5424 - accuracy: 0.7400 - val_loss: 0.4074 - val_accuracy: 0.7778\n",
            "Epoch 167/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5411 - accuracy: 0.7450 - val_loss: 0.4250 - val_accuracy: 0.7407\n",
            "Epoch 168/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5411 - accuracy: 0.7475 - val_loss: 0.4137 - val_accuracy: 0.7778\n",
            "Epoch 169/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5416 - accuracy: 0.7375 - val_loss: 0.3949 - val_accuracy: 0.8148\n",
            "Epoch 170/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5421 - accuracy: 0.7400 - val_loss: 0.4204 - val_accuracy: 0.7778\n",
            "Epoch 171/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5409 - accuracy: 0.7450 - val_loss: 0.4067 - val_accuracy: 0.7778\n",
            "Epoch 172/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5410 - accuracy: 0.7425 - val_loss: 0.4009 - val_accuracy: 0.8148\n",
            "Epoch 173/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5413 - accuracy: 0.7550 - val_loss: 0.4255 - val_accuracy: 0.7037\n",
            "Epoch 174/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5409 - accuracy: 0.7400 - val_loss: 0.4104 - val_accuracy: 0.7778\n",
            "Epoch 175/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.7475 - val_loss: 0.4045 - val_accuracy: 0.8148\n",
            "Epoch 176/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5406 - accuracy: 0.7400 - val_loss: 0.3946 - val_accuracy: 0.8148\n",
            "Epoch 177/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5407 - accuracy: 0.7400 - val_loss: 0.4151 - val_accuracy: 0.7778\n",
            "Epoch 178/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5411 - accuracy: 0.7425 - val_loss: 0.3983 - val_accuracy: 0.8148\n",
            "Epoch 179/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5401 - accuracy: 0.7475 - val_loss: 0.4205 - val_accuracy: 0.7407\n",
            "Epoch 180/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5400 - accuracy: 0.7450 - val_loss: 0.3960 - val_accuracy: 0.8148\n",
            "Epoch 181/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5405 - accuracy: 0.7400 - val_loss: 0.4134 - val_accuracy: 0.7778\n",
            "Epoch 182/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5405 - accuracy: 0.7425 - val_loss: 0.3948 - val_accuracy: 0.8148\n",
            "Epoch 183/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5398 - accuracy: 0.7425 - val_loss: 0.3875 - val_accuracy: 0.8148\n",
            "Epoch 184/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5391 - accuracy: 0.7425 - val_loss: 0.4025 - val_accuracy: 0.8148\n",
            "Epoch 185/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5393 - accuracy: 0.7425 - val_loss: 0.4037 - val_accuracy: 0.8148\n",
            "Epoch 186/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5395 - accuracy: 0.7450 - val_loss: 0.4191 - val_accuracy: 0.7407\n",
            "Epoch 187/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5392 - accuracy: 0.7475 - val_loss: 0.3978 - val_accuracy: 0.8148\n",
            "Epoch 188/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5388 - accuracy: 0.7425 - val_loss: 0.4117 - val_accuracy: 0.8148\n",
            "Epoch 189/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 0.7450 - val_loss: 0.4048 - val_accuracy: 0.8148\n",
            "Epoch 190/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5395 - accuracy: 0.7400 - val_loss: 0.4100 - val_accuracy: 0.7778\n",
            "Epoch 191/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5379 - accuracy: 0.7450 - val_loss: 0.3950 - val_accuracy: 0.8148\n",
            "Epoch 192/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5397 - accuracy: 0.7425 - val_loss: 0.3853 - val_accuracy: 0.8148\n",
            "Epoch 193/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5379 - accuracy: 0.7475 - val_loss: 0.4097 - val_accuracy: 0.7778\n",
            "Epoch 194/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5392 - accuracy: 0.7375 - val_loss: 0.4237 - val_accuracy: 0.7407\n",
            "Epoch 195/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5381 - accuracy: 0.7400 - val_loss: 0.3976 - val_accuracy: 0.8148\n",
            "Epoch 196/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5379 - accuracy: 0.7450 - val_loss: 0.3888 - val_accuracy: 0.8148\n",
            "Epoch 197/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5388 - accuracy: 0.7375 - val_loss: 0.4079 - val_accuracy: 0.7778\n",
            "Epoch 198/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5381 - accuracy: 0.7450 - val_loss: 0.3921 - val_accuracy: 0.8148\n",
            "Epoch 199/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5375 - accuracy: 0.7475 - val_loss: 0.4074 - val_accuracy: 0.7778\n",
            "Epoch 200/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5381 - accuracy: 0.7425 - val_loss: 0.4012 - val_accuracy: 0.8148\n",
            "Epoch 201/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5413 - accuracy: 0.7375 - val_loss: 0.3817 - val_accuracy: 0.8148\n",
            "Epoch 202/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5383 - accuracy: 0.7425 - val_loss: 0.4355 - val_accuracy: 0.7037\n",
            "Epoch 203/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5393 - accuracy: 0.7500 - val_loss: 0.3779 - val_accuracy: 0.8519\n",
            "Epoch 204/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5374 - accuracy: 0.7425 - val_loss: 0.3931 - val_accuracy: 0.8148\n",
            "Epoch 205/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5371 - accuracy: 0.7450 - val_loss: 0.3923 - val_accuracy: 0.8148\n",
            "Epoch 206/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5377 - accuracy: 0.7500 - val_loss: 0.4046 - val_accuracy: 0.7778\n",
            "Epoch 207/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5374 - accuracy: 0.7425 - val_loss: 0.3932 - val_accuracy: 0.8148\n",
            "Epoch 208/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5385 - accuracy: 0.7400 - val_loss: 0.3808 - val_accuracy: 0.8148\n",
            "Epoch 209/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5386 - accuracy: 0.7425 - val_loss: 0.4063 - val_accuracy: 0.7778\n",
            "Epoch 210/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5364 - accuracy: 0.7450 - val_loss: 0.3890 - val_accuracy: 0.8148\n",
            "Epoch 211/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5362 - accuracy: 0.7475 - val_loss: 0.3900 - val_accuracy: 0.8148\n",
            "Epoch 212/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5368 - accuracy: 0.7425 - val_loss: 0.3864 - val_accuracy: 0.8148\n",
            "Epoch 213/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5385 - accuracy: 0.7400 - val_loss: 0.3893 - val_accuracy: 0.8148\n",
            "Epoch 214/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5360 - accuracy: 0.7425 - val_loss: 0.4048 - val_accuracy: 0.7778\n",
            "Epoch 215/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5369 - accuracy: 0.7425 - val_loss: 0.3922 - val_accuracy: 0.8148\n",
            "Epoch 216/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5355 - accuracy: 0.7475 - val_loss: 0.3787 - val_accuracy: 0.8889\n",
            "Epoch 217/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5371 - accuracy: 0.7400 - val_loss: 0.3930 - val_accuracy: 0.8148\n",
            "Epoch 218/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5361 - accuracy: 0.7450 - val_loss: 0.3892 - val_accuracy: 0.8148\n",
            "Epoch 219/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7475 - val_loss: 0.3866 - val_accuracy: 0.8519\n",
            "Epoch 220/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5386 - accuracy: 0.7450 - val_loss: 0.3742 - val_accuracy: 0.8889\n",
            "Epoch 221/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5396 - accuracy: 0.7400 - val_loss: 0.4218 - val_accuracy: 0.7778\n",
            "Epoch 222/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5369 - accuracy: 0.7400 - val_loss: 0.3776 - val_accuracy: 0.8889\n",
            "Epoch 223/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5360 - accuracy: 0.7450 - val_loss: 0.3872 - val_accuracy: 0.8148\n",
            "Epoch 224/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5357 - accuracy: 0.7450 - val_loss: 0.3792 - val_accuracy: 0.8889\n",
            "Epoch 225/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.7450 - val_loss: 0.3888 - val_accuracy: 0.8148\n",
            "Epoch 226/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5346 - accuracy: 0.7450 - val_loss: 0.4038 - val_accuracy: 0.7778\n",
            "Epoch 227/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7425 - val_loss: 0.3983 - val_accuracy: 0.7778\n",
            "Epoch 228/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5369 - accuracy: 0.7450 - val_loss: 0.4070 - val_accuracy: 0.7778\n",
            "Epoch 229/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7425 - val_loss: 0.3587 - val_accuracy: 0.8889\n",
            "Epoch 230/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5368 - accuracy: 0.7375 - val_loss: 0.3633 - val_accuracy: 0.8889\n",
            "Epoch 231/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5335 - accuracy: 0.7400 - val_loss: 0.4048 - val_accuracy: 0.7778\n",
            "Epoch 232/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5359 - accuracy: 0.7400 - val_loss: 0.3998 - val_accuracy: 0.7778\n",
            "Epoch 233/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5365 - accuracy: 0.7425 - val_loss: 0.3660 - val_accuracy: 0.8889\n",
            "Epoch 234/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5364 - accuracy: 0.7425 - val_loss: 0.4005 - val_accuracy: 0.7778\n",
            "Epoch 235/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5344 - accuracy: 0.7425 - val_loss: 0.3894 - val_accuracy: 0.8519\n",
            "Epoch 236/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7475 - val_loss: 0.3756 - val_accuracy: 0.8889\n",
            "Epoch 237/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5337 - accuracy: 0.7450 - val_loss: 0.3883 - val_accuracy: 0.8519\n",
            "Epoch 238/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7475 - val_loss: 0.4005 - val_accuracy: 0.7778\n",
            "Epoch 239/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5375 - accuracy: 0.7425 - val_loss: 0.3658 - val_accuracy: 0.8889\n",
            "Epoch 240/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5343 - accuracy: 0.7450 - val_loss: 0.3903 - val_accuracy: 0.8148\n",
            "Epoch 241/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5342 - accuracy: 0.7450 - val_loss: 0.3891 - val_accuracy: 0.8148\n",
            "Epoch 242/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5332 - accuracy: 0.7475 - val_loss: 0.3705 - val_accuracy: 0.8889\n",
            "Epoch 243/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5353 - accuracy: 0.7450 - val_loss: 0.3622 - val_accuracy: 0.8889\n",
            "Epoch 244/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7375 - val_loss: 0.3981 - val_accuracy: 0.8148\n",
            "Epoch 245/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5331 - accuracy: 0.7425 - val_loss: 0.3833 - val_accuracy: 0.8148\n",
            "Epoch 246/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5359 - accuracy: 0.7475 - val_loss: 0.3657 - val_accuracy: 0.8889\n",
            "Epoch 247/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5352 - accuracy: 0.7500 - val_loss: 0.4050 - val_accuracy: 0.8148\n",
            "Epoch 248/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5332 - accuracy: 0.7450 - val_loss: 0.3775 - val_accuracy: 0.8519\n",
            "Epoch 249/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5335 - accuracy: 0.7475 - val_loss: 0.3686 - val_accuracy: 0.8889\n",
            "Epoch 250/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5336 - accuracy: 0.7425 - val_loss: 0.3845 - val_accuracy: 0.8519\n",
            "Epoch 251/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5329 - accuracy: 0.7425 - val_loss: 0.3929 - val_accuracy: 0.8148\n",
            "Epoch 252/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7475 - val_loss: 0.3779 - val_accuracy: 0.8889\n",
            "Epoch 253/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5336 - accuracy: 0.7450 - val_loss: 0.3905 - val_accuracy: 0.8148\n",
            "Epoch 254/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5360 - accuracy: 0.7425 - val_loss: 0.3530 - val_accuracy: 0.8889\n",
            "Epoch 255/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5317 - accuracy: 0.7400 - val_loss: 0.3797 - val_accuracy: 0.8519\n",
            "Epoch 256/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7475 - val_loss: 0.4005 - val_accuracy: 0.8148\n",
            "Epoch 257/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7475 - val_loss: 0.3625 - val_accuracy: 0.8889\n",
            "Epoch 258/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5327 - accuracy: 0.7475 - val_loss: 0.3622 - val_accuracy: 0.8889\n",
            "Epoch 259/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5318 - accuracy: 0.7525 - val_loss: 0.3841 - val_accuracy: 0.8148\n",
            "Epoch 260/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5340 - accuracy: 0.7450 - val_loss: 0.4001 - val_accuracy: 0.8148\n",
            "Epoch 261/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5318 - accuracy: 0.7425 - val_loss: 0.3738 - val_accuracy: 0.8889\n",
            "Epoch 262/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5320 - accuracy: 0.7475 - val_loss: 0.3599 - val_accuracy: 0.8889\n",
            "Epoch 263/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5325 - accuracy: 0.7450 - val_loss: 0.3626 - val_accuracy: 0.8889\n",
            "Epoch 264/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5318 - accuracy: 0.7400 - val_loss: 0.3773 - val_accuracy: 0.8519\n",
            "Epoch 265/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5319 - accuracy: 0.7450 - val_loss: 0.3747 - val_accuracy: 0.8889\n",
            "Epoch 266/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5311 - accuracy: 0.7475 - val_loss: 0.3698 - val_accuracy: 0.8889\n",
            "Epoch 267/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5319 - accuracy: 0.7450 - val_loss: 0.3718 - val_accuracy: 0.8889\n",
            "Epoch 268/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5314 - accuracy: 0.7450 - val_loss: 0.3676 - val_accuracy: 0.8889\n",
            "Epoch 269/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5311 - accuracy: 0.7425 - val_loss: 0.3782 - val_accuracy: 0.8148\n",
            "Epoch 270/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5323 - accuracy: 0.7425 - val_loss: 0.3833 - val_accuracy: 0.8148\n",
            "Epoch 271/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5310 - accuracy: 0.7475 - val_loss: 0.3642 - val_accuracy: 0.8889\n",
            "Epoch 272/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5311 - accuracy: 0.7475 - val_loss: 0.3819 - val_accuracy: 0.8148\n",
            "Epoch 273/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5319 - accuracy: 0.7400 - val_loss: 0.3669 - val_accuracy: 0.8889\n",
            "Epoch 274/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5309 - accuracy: 0.7450 - val_loss: 0.3586 - val_accuracy: 0.8889\n",
            "Epoch 275/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5307 - accuracy: 0.7450 - val_loss: 0.3701 - val_accuracy: 0.8889\n",
            "Epoch 276/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5312 - accuracy: 0.7425 - val_loss: 0.3704 - val_accuracy: 0.8889\n",
            "Epoch 277/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5310 - accuracy: 0.7400 - val_loss: 0.3741 - val_accuracy: 0.8519\n",
            "Epoch 278/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5307 - accuracy: 0.7475 - val_loss: 0.3795 - val_accuracy: 0.8148\n",
            "Epoch 279/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5299 - accuracy: 0.7475 - val_loss: 0.3634 - val_accuracy: 0.8889\n",
            "Epoch 280/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5317 - accuracy: 0.7450 - val_loss: 0.3495 - val_accuracy: 0.8889\n",
            "Epoch 281/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5315 - accuracy: 0.7500 - val_loss: 0.3903 - val_accuracy: 0.8148\n",
            "Epoch 282/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5300 - accuracy: 0.7450 - val_loss: 0.3572 - val_accuracy: 0.8889\n",
            "Epoch 283/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5302 - accuracy: 0.7475 - val_loss: 0.3710 - val_accuracy: 0.8889\n",
            "Epoch 284/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5298 - accuracy: 0.7500 - val_loss: 0.3666 - val_accuracy: 0.8889\n",
            "Epoch 285/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5295 - accuracy: 0.7425 - val_loss: 0.3551 - val_accuracy: 0.8889\n",
            "Epoch 286/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5295 - accuracy: 0.7525 - val_loss: 0.3643 - val_accuracy: 0.8889\n",
            "Epoch 287/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5292 - accuracy: 0.7450 - val_loss: 0.3837 - val_accuracy: 0.8148\n",
            "Epoch 288/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5319 - accuracy: 0.7375 - val_loss: 0.3707 - val_accuracy: 0.8519\n",
            "Epoch 289/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5300 - accuracy: 0.7500 - val_loss: 0.3795 - val_accuracy: 0.8148\n",
            "Epoch 290/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5310 - accuracy: 0.7500 - val_loss: 0.3532 - val_accuracy: 0.8889\n",
            "Epoch 291/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5292 - accuracy: 0.7500 - val_loss: 0.3676 - val_accuracy: 0.8889\n",
            "Epoch 292/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5284 - accuracy: 0.7475 - val_loss: 0.3602 - val_accuracy: 0.8889\n",
            "Epoch 293/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5295 - accuracy: 0.7450 - val_loss: 0.3514 - val_accuracy: 0.8889\n",
            "Epoch 294/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5279 - accuracy: 0.7525 - val_loss: 0.3752 - val_accuracy: 0.8519\n",
            "Epoch 295/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5286 - accuracy: 0.7500 - val_loss: 0.3663 - val_accuracy: 0.8889\n",
            "Epoch 296/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5287 - accuracy: 0.7575 - val_loss: 0.3664 - val_accuracy: 0.8889\n",
            "Epoch 297/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5281 - accuracy: 0.7475 - val_loss: 0.3716 - val_accuracy: 0.8519\n",
            "Epoch 298/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5282 - accuracy: 0.7500 - val_loss: 0.3646 - val_accuracy: 0.8889\n",
            "Epoch 299/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5290 - accuracy: 0.7475 - val_loss: 0.3770 - val_accuracy: 0.8148\n",
            "Epoch 300/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5275 - accuracy: 0.7475 - val_loss: 0.3572 - val_accuracy: 0.8889\n",
            "Epoch 301/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5285 - accuracy: 0.7475 - val_loss: 0.3386 - val_accuracy: 0.9259\n",
            "Epoch 302/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5290 - accuracy: 0.7525 - val_loss: 0.3551 - val_accuracy: 0.8889\n",
            "Epoch 303/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5295 - accuracy: 0.7450 - val_loss: 0.3780 - val_accuracy: 0.8148\n",
            "Epoch 304/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5275 - accuracy: 0.7475 - val_loss: 0.3538 - val_accuracy: 0.8889\n",
            "Epoch 305/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5277 - accuracy: 0.7575 - val_loss: 0.3424 - val_accuracy: 0.9259\n",
            "Epoch 306/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5285 - accuracy: 0.7575 - val_loss: 0.3536 - val_accuracy: 0.8889\n",
            "Epoch 307/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5271 - accuracy: 0.7550 - val_loss: 0.3596 - val_accuracy: 0.8889\n",
            "Epoch 308/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5271 - accuracy: 0.7575 - val_loss: 0.3575 - val_accuracy: 0.8889\n",
            "Epoch 309/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5282 - accuracy: 0.7500 - val_loss: 0.3700 - val_accuracy: 0.8519\n",
            "Epoch 310/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5277 - accuracy: 0.7500 - val_loss: 0.3613 - val_accuracy: 0.8519\n",
            "Epoch 311/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5271 - accuracy: 0.7550 - val_loss: 0.3359 - val_accuracy: 0.9259\n",
            "Epoch 312/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5293 - accuracy: 0.7400 - val_loss: 0.3692 - val_accuracy: 0.8519\n",
            "Epoch 313/800\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.5267 - accuracy: 0.7525 - val_loss: 0.3575 - val_accuracy: 0.8889\n",
            "Epoch 314/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5288 - accuracy: 0.7525 - val_loss: 0.3496 - val_accuracy: 0.8889\n",
            "Epoch 315/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5277 - accuracy: 0.7500 - val_loss: 0.3693 - val_accuracy: 0.8519\n",
            "Epoch 316/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5267 - accuracy: 0.7475 - val_loss: 0.3631 - val_accuracy: 0.8519\n",
            "Epoch 317/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5289 - accuracy: 0.7400 - val_loss: 0.3363 - val_accuracy: 0.9259\n",
            "Epoch 318/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5281 - accuracy: 0.7575 - val_loss: 0.3793 - val_accuracy: 0.8148\n",
            "Epoch 319/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5262 - accuracy: 0.7550 - val_loss: 0.3538 - val_accuracy: 0.8889\n",
            "Epoch 320/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5263 - accuracy: 0.7550 - val_loss: 0.3400 - val_accuracy: 0.9259\n",
            "Epoch 321/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5260 - accuracy: 0.7575 - val_loss: 0.3549 - val_accuracy: 0.8889\n",
            "Epoch 322/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5270 - accuracy: 0.7525 - val_loss: 0.3695 - val_accuracy: 0.8519\n",
            "Epoch 323/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5263 - accuracy: 0.7550 - val_loss: 0.3439 - val_accuracy: 0.8889\n",
            "Epoch 324/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5267 - accuracy: 0.7525 - val_loss: 0.3483 - val_accuracy: 0.8889\n",
            "Epoch 325/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5262 - accuracy: 0.7500 - val_loss: 0.3734 - val_accuracy: 0.8148\n",
            "Epoch 326/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5253 - accuracy: 0.7525 - val_loss: 0.3528 - val_accuracy: 0.8889\n",
            "Epoch 327/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5263 - accuracy: 0.7550 - val_loss: 0.3352 - val_accuracy: 0.9259\n",
            "Epoch 328/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5267 - accuracy: 0.7550 - val_loss: 0.3743 - val_accuracy: 0.8148\n",
            "Epoch 329/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5254 - accuracy: 0.7525 - val_loss: 0.3514 - val_accuracy: 0.8889\n",
            "Epoch 330/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5260 - accuracy: 0.7550 - val_loss: 0.3509 - val_accuracy: 0.8889\n",
            "Epoch 331/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5251 - accuracy: 0.7525 - val_loss: 0.3372 - val_accuracy: 0.9259\n",
            "Epoch 332/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5262 - accuracy: 0.7550 - val_loss: 0.3590 - val_accuracy: 0.8519\n",
            "Epoch 333/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5248 - accuracy: 0.7550 - val_loss: 0.3545 - val_accuracy: 0.8889\n",
            "Epoch 334/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5246 - accuracy: 0.7550 - val_loss: 0.3491 - val_accuracy: 0.8889\n",
            "Epoch 335/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5259 - accuracy: 0.7550 - val_loss: 0.3623 - val_accuracy: 0.8519\n",
            "Epoch 336/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5254 - accuracy: 0.7550 - val_loss: 0.3453 - val_accuracy: 0.8889\n",
            "Epoch 337/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5247 - accuracy: 0.7500 - val_loss: 0.3517 - val_accuracy: 0.8889\n",
            "Epoch 338/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5250 - accuracy: 0.7550 - val_loss: 0.3460 - val_accuracy: 0.9259\n",
            "Epoch 339/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5238 - accuracy: 0.7500 - val_loss: 0.3592 - val_accuracy: 0.8519\n",
            "Epoch 340/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5248 - accuracy: 0.7550 - val_loss: 0.3619 - val_accuracy: 0.8519\n",
            "Epoch 341/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5246 - accuracy: 0.7525 - val_loss: 0.3464 - val_accuracy: 0.9259\n",
            "Epoch 342/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5247 - accuracy: 0.7500 - val_loss: 0.3568 - val_accuracy: 0.8889\n",
            "Epoch 343/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5241 - accuracy: 0.7525 - val_loss: 0.3359 - val_accuracy: 0.9259\n",
            "Epoch 344/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5245 - accuracy: 0.7625 - val_loss: 0.3498 - val_accuracy: 0.8889\n",
            "Epoch 345/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5256 - accuracy: 0.7500 - val_loss: 0.3565 - val_accuracy: 0.8889\n",
            "Epoch 346/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5235 - accuracy: 0.7550 - val_loss: 0.3381 - val_accuracy: 0.9259\n",
            "Epoch 347/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5249 - accuracy: 0.7475 - val_loss: 0.3256 - val_accuracy: 0.9259\n",
            "Epoch 348/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5244 - accuracy: 0.7525 - val_loss: 0.3641 - val_accuracy: 0.8519\n",
            "Epoch 349/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5247 - accuracy: 0.7550 - val_loss: 0.3444 - val_accuracy: 0.9259\n",
            "Epoch 350/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5240 - accuracy: 0.7550 - val_loss: 0.3466 - val_accuracy: 0.9259\n",
            "Epoch 351/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.7550 - val_loss: 0.3485 - val_accuracy: 0.9259\n",
            "Epoch 352/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5225 - accuracy: 0.7525 - val_loss: 0.3602 - val_accuracy: 0.8519\n",
            "Epoch 353/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5231 - accuracy: 0.7550 - val_loss: 0.3542 - val_accuracy: 0.8889\n",
            "Epoch 354/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5250 - accuracy: 0.7600 - val_loss: 0.3324 - val_accuracy: 0.9259\n",
            "Epoch 355/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5239 - accuracy: 0.7550 - val_loss: 0.3506 - val_accuracy: 0.8889\n",
            "Epoch 356/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5234 - accuracy: 0.7475 - val_loss: 0.3661 - val_accuracy: 0.8519\n",
            "Epoch 357/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5233 - accuracy: 0.7475 - val_loss: 0.3407 - val_accuracy: 0.9259\n",
            "Epoch 358/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5235 - accuracy: 0.7550 - val_loss: 0.3242 - val_accuracy: 0.9259\n",
            "Epoch 359/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5220 - accuracy: 0.7550 - val_loss: 0.3527 - val_accuracy: 0.8889\n",
            "Epoch 360/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5243 - accuracy: 0.7450 - val_loss: 0.3643 - val_accuracy: 0.8519\n",
            "Epoch 361/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5228 - accuracy: 0.7575 - val_loss: 0.3242 - val_accuracy: 0.9259\n",
            "Epoch 362/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5225 - accuracy: 0.7575 - val_loss: 0.3448 - val_accuracy: 0.9259\n",
            "Epoch 363/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5225 - accuracy: 0.7525 - val_loss: 0.3434 - val_accuracy: 0.9259\n",
            "Epoch 364/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5222 - accuracy: 0.7500 - val_loss: 0.3541 - val_accuracy: 0.8519\n",
            "Epoch 365/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5210 - accuracy: 0.7525 - val_loss: 0.3340 - val_accuracy: 0.9259\n",
            "Epoch 366/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7475 - val_loss: 0.3387 - val_accuracy: 0.9259\n",
            "Epoch 367/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5236 - accuracy: 0.7500 - val_loss: 0.3227 - val_accuracy: 0.9259\n",
            "Epoch 368/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5233 - accuracy: 0.7450 - val_loss: 0.3644 - val_accuracy: 0.8519\n",
            "Epoch 369/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5220 - accuracy: 0.7575 - val_loss: 0.3258 - val_accuracy: 0.9259\n",
            "Epoch 370/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5232 - accuracy: 0.7500 - val_loss: 0.3177 - val_accuracy: 0.9259\n",
            "Epoch 371/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5194 - accuracy: 0.7550 - val_loss: 0.3561 - val_accuracy: 0.8519\n",
            "Epoch 372/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5226 - accuracy: 0.7500 - val_loss: 0.3407 - val_accuracy: 0.9259\n",
            "Epoch 373/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5247 - accuracy: 0.7400 - val_loss: 0.3580 - val_accuracy: 0.8519\n",
            "Epoch 374/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5191 - accuracy: 0.7600 - val_loss: 0.3092 - val_accuracy: 0.9259\n",
            "Epoch 375/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5223 - accuracy: 0.7525 - val_loss: 0.3254 - val_accuracy: 0.9259\n",
            "Epoch 376/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5214 - accuracy: 0.7550 - val_loss: 0.3437 - val_accuracy: 0.9259\n",
            "Epoch 377/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.7500 - val_loss: 0.3477 - val_accuracy: 0.8889\n",
            "Epoch 378/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.7500 - val_loss: 0.3217 - val_accuracy: 0.9259\n",
            "Epoch 379/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5223 - accuracy: 0.7575 - val_loss: 0.3186 - val_accuracy: 0.9259\n",
            "Epoch 380/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5210 - accuracy: 0.7575 - val_loss: 0.3505 - val_accuracy: 0.8889\n",
            "Epoch 381/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5215 - accuracy: 0.7525 - val_loss: 0.3236 - val_accuracy: 0.9259\n",
            "Epoch 382/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5205 - accuracy: 0.7500 - val_loss: 0.3316 - val_accuracy: 0.9259\n",
            "Epoch 383/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5199 - accuracy: 0.7500 - val_loss: 0.3364 - val_accuracy: 0.9259\n",
            "Epoch 384/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5200 - accuracy: 0.7500 - val_loss: 0.3464 - val_accuracy: 0.8889\n",
            "Epoch 385/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5202 - accuracy: 0.7500 - val_loss: 0.3301 - val_accuracy: 0.9259\n",
            "Epoch 386/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5200 - accuracy: 0.7550 - val_loss: 0.3460 - val_accuracy: 0.8889\n",
            "Epoch 387/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5206 - accuracy: 0.7500 - val_loss: 0.3213 - val_accuracy: 0.9259\n",
            "Epoch 388/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5200 - accuracy: 0.7575 - val_loss: 0.3277 - val_accuracy: 0.9259\n",
            "Epoch 389/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5189 - accuracy: 0.7475 - val_loss: 0.3372 - val_accuracy: 0.9259\n",
            "Epoch 390/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5204 - accuracy: 0.7425 - val_loss: 0.3442 - val_accuracy: 0.8889\n",
            "Epoch 391/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5184 - accuracy: 0.7475 - val_loss: 0.3170 - val_accuracy: 0.9259\n",
            "Epoch 392/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5192 - accuracy: 0.7500 - val_loss: 0.3370 - val_accuracy: 0.8889\n",
            "Epoch 393/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5185 - accuracy: 0.7450 - val_loss: 0.3252 - val_accuracy: 0.9259\n",
            "Epoch 394/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5195 - accuracy: 0.7500 - val_loss: 0.3268 - val_accuracy: 0.9259\n",
            "Epoch 395/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5181 - accuracy: 0.7475 - val_loss: 0.3310 - val_accuracy: 0.9259\n",
            "Epoch 396/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5216 - accuracy: 0.7450 - val_loss: 0.3438 - val_accuracy: 0.8889\n",
            "Epoch 397/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5184 - accuracy: 0.7550 - val_loss: 0.3147 - val_accuracy: 0.9259\n",
            "Epoch 398/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5207 - accuracy: 0.7650 - val_loss: 0.3204 - val_accuracy: 0.9259\n",
            "Epoch 399/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5225 - accuracy: 0.7475 - val_loss: 0.3583 - val_accuracy: 0.8519\n",
            "Epoch 400/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5184 - accuracy: 0.7500 - val_loss: 0.3100 - val_accuracy: 0.9259\n",
            "Epoch 401/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5184 - accuracy: 0.7525 - val_loss: 0.3196 - val_accuracy: 0.9259\n",
            "Epoch 402/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5184 - accuracy: 0.7475 - val_loss: 0.3333 - val_accuracy: 0.9259\n",
            "Epoch 403/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5175 - accuracy: 0.7450 - val_loss: 0.3195 - val_accuracy: 0.9259\n",
            "Epoch 404/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5174 - accuracy: 0.7500 - val_loss: 0.3199 - val_accuracy: 0.9259\n",
            "Epoch 405/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5176 - accuracy: 0.7475 - val_loss: 0.3294 - val_accuracy: 0.9259\n",
            "Epoch 406/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5172 - accuracy: 0.7450 - val_loss: 0.3189 - val_accuracy: 0.9259\n",
            "Epoch 407/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5175 - accuracy: 0.7475 - val_loss: 0.3292 - val_accuracy: 0.9259\n",
            "Epoch 408/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5167 - accuracy: 0.7550 - val_loss: 0.3160 - val_accuracy: 0.9259\n",
            "Epoch 409/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5170 - accuracy: 0.7500 - val_loss: 0.3211 - val_accuracy: 0.9259\n",
            "Epoch 410/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5169 - accuracy: 0.7475 - val_loss: 0.3237 - val_accuracy: 0.9259\n",
            "Epoch 411/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5189 - accuracy: 0.7450 - val_loss: 0.3227 - val_accuracy: 0.9259\n",
            "Epoch 412/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.7450 - val_loss: 0.3262 - val_accuracy: 0.9259\n",
            "Epoch 413/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5164 - accuracy: 0.7450 - val_loss: 0.3214 - val_accuracy: 0.9259\n",
            "Epoch 414/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5194 - accuracy: 0.7400 - val_loss: 0.3300 - val_accuracy: 0.9259\n",
            "Epoch 415/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5159 - accuracy: 0.7500 - val_loss: 0.3080 - val_accuracy: 0.9259\n",
            "Epoch 416/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5182 - accuracy: 0.7650 - val_loss: 0.2969 - val_accuracy: 0.9259\n",
            "Epoch 417/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5157 - accuracy: 0.7525 - val_loss: 0.3370 - val_accuracy: 0.8889\n",
            "Epoch 418/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5178 - accuracy: 0.7475 - val_loss: 0.3304 - val_accuracy: 0.9259\n",
            "Epoch 419/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5159 - accuracy: 0.7425 - val_loss: 0.3125 - val_accuracy: 0.9259\n",
            "Epoch 420/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5163 - accuracy: 0.7550 - val_loss: 0.3045 - val_accuracy: 0.9259\n",
            "Epoch 421/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5145 - accuracy: 0.7500 - val_loss: 0.3273 - val_accuracy: 0.9259\n",
            "Epoch 422/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5161 - accuracy: 0.7400 - val_loss: 0.3308 - val_accuracy: 0.9259\n",
            "Epoch 423/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5155 - accuracy: 0.7450 - val_loss: 0.3190 - val_accuracy: 0.9259\n",
            "Epoch 424/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5181 - accuracy: 0.7575 - val_loss: 0.2988 - val_accuracy: 0.9259\n",
            "Epoch 425/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5149 - accuracy: 0.7525 - val_loss: 0.3332 - val_accuracy: 0.9259\n",
            "Epoch 426/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5177 - accuracy: 0.7450 - val_loss: 0.3307 - val_accuracy: 0.9259\n",
            "Epoch 427/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5153 - accuracy: 0.7525 - val_loss: 0.2992 - val_accuracy: 0.9259\n",
            "Epoch 428/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5154 - accuracy: 0.7500 - val_loss: 0.3188 - val_accuracy: 0.9259\n",
            "Epoch 429/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5151 - accuracy: 0.7450 - val_loss: 0.3153 - val_accuracy: 0.9259\n",
            "Epoch 430/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5155 - accuracy: 0.7575 - val_loss: 0.3087 - val_accuracy: 0.9259\n",
            "Epoch 431/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5150 - accuracy: 0.7425 - val_loss: 0.3239 - val_accuracy: 0.9259\n",
            "Epoch 432/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5145 - accuracy: 0.7450 - val_loss: 0.3015 - val_accuracy: 0.9259\n",
            "Epoch 433/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5149 - accuracy: 0.7550 - val_loss: 0.3184 - val_accuracy: 0.9259\n",
            "Epoch 434/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5189 - accuracy: 0.7525 - val_loss: 0.3020 - val_accuracy: 0.9259\n",
            "Epoch 435/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5142 - accuracy: 0.7500 - val_loss: 0.3413 - val_accuracy: 0.8889\n",
            "Epoch 436/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5158 - accuracy: 0.7475 - val_loss: 0.3019 - val_accuracy: 0.9259\n",
            "Epoch 437/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5137 - accuracy: 0.7550 - val_loss: 0.3161 - val_accuracy: 0.9259\n",
            "Epoch 438/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5153 - accuracy: 0.7525 - val_loss: 0.3073 - val_accuracy: 0.9259\n",
            "Epoch 439/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5137 - accuracy: 0.7475 - val_loss: 0.3214 - val_accuracy: 0.9259\n",
            "Epoch 440/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5137 - accuracy: 0.7450 - val_loss: 0.3111 - val_accuracy: 0.9259\n",
            "Epoch 441/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5135 - accuracy: 0.7500 - val_loss: 0.3197 - val_accuracy: 0.9259\n",
            "Epoch 442/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5140 - accuracy: 0.7600 - val_loss: 0.2958 - val_accuracy: 0.9630\n",
            "Epoch 443/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5135 - accuracy: 0.7475 - val_loss: 0.3129 - val_accuracy: 0.9259\n",
            "Epoch 444/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5134 - accuracy: 0.7450 - val_loss: 0.3188 - val_accuracy: 0.9259\n",
            "Epoch 445/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5135 - accuracy: 0.7450 - val_loss: 0.3024 - val_accuracy: 0.9259\n",
            "Epoch 446/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5127 - accuracy: 0.7475 - val_loss: 0.3071 - val_accuracy: 0.9259\n",
            "Epoch 447/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5135 - accuracy: 0.7450 - val_loss: 0.3071 - val_accuracy: 0.9259\n",
            "Epoch 448/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5131 - accuracy: 0.7500 - val_loss: 0.3058 - val_accuracy: 0.9259\n",
            "Epoch 449/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5133 - accuracy: 0.7525 - val_loss: 0.2993 - val_accuracy: 0.9259\n",
            "Epoch 450/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5125 - accuracy: 0.7500 - val_loss: 0.3177 - val_accuracy: 0.9259\n",
            "Epoch 451/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5125 - accuracy: 0.7475 - val_loss: 0.3071 - val_accuracy: 0.9259\n",
            "Epoch 452/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5146 - accuracy: 0.7450 - val_loss: 0.3131 - val_accuracy: 0.9259\n",
            "Epoch 453/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5124 - accuracy: 0.7525 - val_loss: 0.2822 - val_accuracy: 0.9630\n",
            "Epoch 454/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5123 - accuracy: 0.7450 - val_loss: 0.3096 - val_accuracy: 0.9259\n",
            "Epoch 455/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5142 - accuracy: 0.7400 - val_loss: 0.3088 - val_accuracy: 0.9259\n",
            "Epoch 456/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5121 - accuracy: 0.7450 - val_loss: 0.3087 - val_accuracy: 0.9259\n",
            "Epoch 457/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5116 - accuracy: 0.7500 - val_loss: 0.2862 - val_accuracy: 0.9630\n",
            "Epoch 458/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5128 - accuracy: 0.7400 - val_loss: 0.3092 - val_accuracy: 0.9259\n",
            "Epoch 459/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5141 - accuracy: 0.7475 - val_loss: 0.2937 - val_accuracy: 0.9630\n",
            "Epoch 460/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5110 - accuracy: 0.7500 - val_loss: 0.3149 - val_accuracy: 0.9259\n",
            "Epoch 461/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5111 - accuracy: 0.7450 - val_loss: 0.2958 - val_accuracy: 0.9630\n",
            "Epoch 462/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5120 - accuracy: 0.7575 - val_loss: 0.2952 - val_accuracy: 0.9630\n",
            "Epoch 463/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5108 - accuracy: 0.7425 - val_loss: 0.3026 - val_accuracy: 0.9259\n",
            "Epoch 464/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5107 - accuracy: 0.7475 - val_loss: 0.3089 - val_accuracy: 0.9259\n",
            "Epoch 465/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5131 - accuracy: 0.7550 - val_loss: 0.3087 - val_accuracy: 0.9259\n",
            "Epoch 466/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5112 - accuracy: 0.7550 - val_loss: 0.2947 - val_accuracy: 0.9630\n",
            "Epoch 467/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5124 - accuracy: 0.7625 - val_loss: 0.2989 - val_accuracy: 0.9630\n",
            "Epoch 468/800\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.5107 - accuracy: 0.7475 - val_loss: 0.3020 - val_accuracy: 0.9259\n",
            "Epoch 469/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5109 - accuracy: 0.7475 - val_loss: 0.3038 - val_accuracy: 0.9259\n",
            "Epoch 470/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5110 - accuracy: 0.7450 - val_loss: 0.2931 - val_accuracy: 0.9630\n",
            "Epoch 471/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5105 - accuracy: 0.7525 - val_loss: 0.3046 - val_accuracy: 0.9259\n",
            "Epoch 472/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5137 - accuracy: 0.7625 - val_loss: 0.2821 - val_accuracy: 0.9630\n",
            "Epoch 473/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5106 - accuracy: 0.7550 - val_loss: 0.3040 - val_accuracy: 0.9259\n",
            "Epoch 474/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5095 - accuracy: 0.7475 - val_loss: 0.2970 - val_accuracy: 0.9630\n",
            "Epoch 475/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5096 - accuracy: 0.7475 - val_loss: 0.2943 - val_accuracy: 0.9630\n",
            "Epoch 476/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5096 - accuracy: 0.7525 - val_loss: 0.2907 - val_accuracy: 0.9630\n",
            "Epoch 477/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.7500 - val_loss: 0.2990 - val_accuracy: 0.9630\n",
            "Epoch 478/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5095 - accuracy: 0.7500 - val_loss: 0.2904 - val_accuracy: 0.9630\n",
            "Epoch 479/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5101 - accuracy: 0.7600 - val_loss: 0.2787 - val_accuracy: 0.9630\n",
            "Epoch 480/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5111 - accuracy: 0.7450 - val_loss: 0.2988 - val_accuracy: 0.9630\n",
            "Epoch 481/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5105 - accuracy: 0.7550 - val_loss: 0.2831 - val_accuracy: 0.9630\n",
            "Epoch 482/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5083 - accuracy: 0.7500 - val_loss: 0.2966 - val_accuracy: 0.9630\n",
            "Epoch 483/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5084 - accuracy: 0.7475 - val_loss: 0.3020 - val_accuracy: 0.9630\n",
            "Epoch 484/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7500 - val_loss: 0.2902 - val_accuracy: 0.9630\n",
            "Epoch 485/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5100 - accuracy: 0.7550 - val_loss: 0.2877 - val_accuracy: 0.9630\n",
            "Epoch 486/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5082 - accuracy: 0.7575 - val_loss: 0.2901 - val_accuracy: 0.9630\n",
            "Epoch 487/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5112 - accuracy: 0.7550 - val_loss: 0.2819 - val_accuracy: 0.9630\n",
            "Epoch 488/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7450 - val_loss: 0.3130 - val_accuracy: 0.9259\n",
            "Epoch 489/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5114 - accuracy: 0.7625 - val_loss: 0.2807 - val_accuracy: 0.9630\n",
            "Epoch 490/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5078 - accuracy: 0.7550 - val_loss: 0.3059 - val_accuracy: 0.9259\n",
            "Epoch 491/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5086 - accuracy: 0.7450 - val_loss: 0.2923 - val_accuracy: 0.9630\n",
            "Epoch 492/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5091 - accuracy: 0.7550 - val_loss: 0.2790 - val_accuracy: 0.9630\n",
            "Epoch 493/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5087 - accuracy: 0.7525 - val_loss: 0.2998 - val_accuracy: 0.9630\n",
            "Epoch 494/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5084 - accuracy: 0.7525 - val_loss: 0.2774 - val_accuracy: 0.9630\n",
            "Epoch 495/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5098 - accuracy: 0.7475 - val_loss: 0.2990 - val_accuracy: 0.9630\n",
            "Epoch 496/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5096 - accuracy: 0.7675 - val_loss: 0.2723 - val_accuracy: 0.9630\n",
            "Epoch 497/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5079 - accuracy: 0.7575 - val_loss: 0.2852 - val_accuracy: 0.9630\n",
            "Epoch 498/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5098 - accuracy: 0.7550 - val_loss: 0.2974 - val_accuracy: 0.9630\n",
            "Epoch 499/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.7600 - val_loss: 0.2621 - val_accuracy: 0.9630\n",
            "Epoch 500/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5126 - accuracy: 0.7450 - val_loss: 0.2966 - val_accuracy: 0.9630\n",
            "Epoch 501/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5099 - accuracy: 0.7650 - val_loss: 0.2689 - val_accuracy: 0.9630\n",
            "Epoch 502/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5114 - accuracy: 0.7525 - val_loss: 0.2910 - val_accuracy: 0.9630\n",
            "Epoch 503/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5144 - accuracy: 0.7625 - val_loss: 0.2643 - val_accuracy: 0.9630\n",
            "Epoch 504/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5078 - accuracy: 0.7450 - val_loss: 0.2989 - val_accuracy: 0.9630\n",
            "Epoch 505/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5072 - accuracy: 0.7475 - val_loss: 0.2820 - val_accuracy: 0.9630\n",
            "Epoch 506/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5099 - accuracy: 0.7675 - val_loss: 0.2742 - val_accuracy: 0.9630\n",
            "Epoch 507/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5079 - accuracy: 0.7650 - val_loss: 0.2793 - val_accuracy: 0.9630\n",
            "Epoch 508/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5057 - accuracy: 0.7600 - val_loss: 0.2910 - val_accuracy: 0.9630\n",
            "Epoch 509/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5077 - accuracy: 0.7550 - val_loss: 0.2905 - val_accuracy: 0.9630\n",
            "Epoch 510/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5065 - accuracy: 0.7525 - val_loss: 0.2875 - val_accuracy: 0.9630\n",
            "Epoch 511/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5069 - accuracy: 0.7525 - val_loss: 0.2682 - val_accuracy: 0.9630\n",
            "Epoch 512/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5087 - accuracy: 0.7575 - val_loss: 0.2869 - val_accuracy: 0.9630\n",
            "Epoch 513/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5054 - accuracy: 0.7575 - val_loss: 0.2693 - val_accuracy: 0.9630\n",
            "Epoch 514/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5076 - accuracy: 0.7675 - val_loss: 0.2678 - val_accuracy: 0.9630\n",
            "Epoch 515/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5052 - accuracy: 0.7550 - val_loss: 0.2923 - val_accuracy: 0.9630\n",
            "Epoch 516/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5059 - accuracy: 0.7575 - val_loss: 0.2755 - val_accuracy: 0.9630\n",
            "Epoch 517/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5085 - accuracy: 0.7625 - val_loss: 0.2695 - val_accuracy: 0.9630\n",
            "Epoch 518/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5061 - accuracy: 0.7575 - val_loss: 0.2909 - val_accuracy: 0.9630\n",
            "Epoch 519/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5063 - accuracy: 0.7525 - val_loss: 0.2735 - val_accuracy: 0.9630\n",
            "Epoch 520/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5059 - accuracy: 0.7625 - val_loss: 0.2736 - val_accuracy: 0.9630\n",
            "Epoch 521/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5054 - accuracy: 0.7575 - val_loss: 0.2765 - val_accuracy: 0.9630\n",
            "Epoch 522/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5059 - accuracy: 0.7550 - val_loss: 0.2762 - val_accuracy: 0.9630\n",
            "Epoch 523/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5052 - accuracy: 0.7600 - val_loss: 0.2743 - val_accuracy: 0.9630\n",
            "Epoch 524/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7575 - val_loss: 0.2773 - val_accuracy: 0.9630\n",
            "Epoch 525/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5058 - accuracy: 0.7575 - val_loss: 0.2814 - val_accuracy: 0.9630\n",
            "Epoch 526/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5048 - accuracy: 0.7625 - val_loss: 0.2728 - val_accuracy: 0.9630\n",
            "Epoch 527/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5085 - accuracy: 0.7575 - val_loss: 0.2655 - val_accuracy: 0.9630\n",
            "Epoch 528/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5053 - accuracy: 0.7575 - val_loss: 0.2937 - val_accuracy: 0.9630\n",
            "Epoch 529/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5048 - accuracy: 0.7625 - val_loss: 0.2628 - val_accuracy: 0.9630\n",
            "Epoch 530/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5048 - accuracy: 0.7625 - val_loss: 0.2696 - val_accuracy: 0.9630\n",
            "Epoch 531/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5056 - accuracy: 0.7650 - val_loss: 0.2861 - val_accuracy: 0.9630\n",
            "Epoch 532/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5046 - accuracy: 0.7675 - val_loss: 0.2628 - val_accuracy: 0.9630\n",
            "Epoch 533/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5048 - accuracy: 0.7750 - val_loss: 0.2685 - val_accuracy: 0.9630\n",
            "Epoch 534/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5062 - accuracy: 0.7625 - val_loss: 0.2880 - val_accuracy: 0.9630\n",
            "Epoch 535/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5028 - accuracy: 0.7600 - val_loss: 0.2578 - val_accuracy: 0.9630\n",
            "Epoch 536/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5066 - accuracy: 0.7650 - val_loss: 0.2611 - val_accuracy: 0.9630\n",
            "Epoch 537/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5047 - accuracy: 0.7550 - val_loss: 0.2791 - val_accuracy: 0.9630\n",
            "Epoch 538/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5074 - accuracy: 0.7625 - val_loss: 0.2530 - val_accuracy: 0.9630\n",
            "Epoch 539/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5069 - accuracy: 0.7550 - val_loss: 0.2862 - val_accuracy: 0.9630\n",
            "Epoch 540/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5025 - accuracy: 0.7700 - val_loss: 0.2515 - val_accuracy: 0.9630\n",
            "Epoch 541/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5063 - accuracy: 0.7750 - val_loss: 0.2680 - val_accuracy: 0.9630\n",
            "Epoch 542/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5043 - accuracy: 0.7550 - val_loss: 0.2893 - val_accuracy: 0.9630\n",
            "Epoch 543/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5040 - accuracy: 0.7575 - val_loss: 0.2641 - val_accuracy: 0.9630\n",
            "Epoch 544/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5035 - accuracy: 0.7650 - val_loss: 0.2547 - val_accuracy: 0.9630\n",
            "Epoch 545/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5087 - accuracy: 0.7750 - val_loss: 0.2809 - val_accuracy: 0.9630\n",
            "Epoch 546/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5085 - accuracy: 0.7750 - val_loss: 0.2493 - val_accuracy: 0.9630\n",
            "Epoch 547/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5030 - accuracy: 0.7625 - val_loss: 0.2854 - val_accuracy: 0.9630\n",
            "Epoch 548/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5050 - accuracy: 0.7625 - val_loss: 0.2704 - val_accuracy: 0.9630\n",
            "Epoch 549/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5032 - accuracy: 0.7650 - val_loss: 0.2645 - val_accuracy: 0.9630\n",
            "Epoch 550/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5064 - accuracy: 0.7625 - val_loss: 0.2761 - val_accuracy: 0.9630\n",
            "Epoch 551/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.7625 - val_loss: 0.2593 - val_accuracy: 0.9630\n",
            "Epoch 552/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5059 - accuracy: 0.7700 - val_loss: 0.2571 - val_accuracy: 0.9630\n",
            "Epoch 553/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5043 - accuracy: 0.7550 - val_loss: 0.2790 - val_accuracy: 0.9630\n",
            "Epoch 554/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5049 - accuracy: 0.7675 - val_loss: 0.2683 - val_accuracy: 0.9630\n",
            "Epoch 555/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5054 - accuracy: 0.7775 - val_loss: 0.2473 - val_accuracy: 0.9630\n",
            "Epoch 556/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5035 - accuracy: 0.7650 - val_loss: 0.2821 - val_accuracy: 0.9630\n",
            "Epoch 557/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5052 - accuracy: 0.7675 - val_loss: 0.2640 - val_accuracy: 0.9630\n",
            "Epoch 558/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5031 - accuracy: 0.7675 - val_loss: 0.2589 - val_accuracy: 0.9630\n",
            "Epoch 559/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5042 - accuracy: 0.7650 - val_loss: 0.2655 - val_accuracy: 0.9630\n",
            "Epoch 560/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5037 - accuracy: 0.7725 - val_loss: 0.2521 - val_accuracy: 0.9630\n",
            "Epoch 561/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5046 - accuracy: 0.7725 - val_loss: 0.2729 - val_accuracy: 0.9630\n",
            "Epoch 562/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5026 - accuracy: 0.7650 - val_loss: 0.2598 - val_accuracy: 0.9630\n",
            "Epoch 563/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5033 - accuracy: 0.7575 - val_loss: 0.2616 - val_accuracy: 0.9630\n",
            "Epoch 564/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5033 - accuracy: 0.7650 - val_loss: 0.2666 - val_accuracy: 0.9630\n",
            "Epoch 565/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5041 - accuracy: 0.7650 - val_loss: 0.2544 - val_accuracy: 0.9630\n",
            "Epoch 566/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5037 - accuracy: 0.7575 - val_loss: 0.2688 - val_accuracy: 0.9630\n",
            "Epoch 567/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5059 - accuracy: 0.7650 - val_loss: 0.2555 - val_accuracy: 0.9630\n",
            "Epoch 568/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.7700 - val_loss: 0.2678 - val_accuracy: 0.9630\n",
            "Epoch 569/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5022 - accuracy: 0.7625 - val_loss: 0.2540 - val_accuracy: 0.9630\n",
            "Epoch 570/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.7675 - val_loss: 0.2700 - val_accuracy: 0.9630\n",
            "Epoch 571/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5017 - accuracy: 0.7675 - val_loss: 0.2523 - val_accuracy: 0.9630\n",
            "Epoch 572/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5040 - accuracy: 0.7700 - val_loss: 0.2650 - val_accuracy: 0.9630\n",
            "Epoch 573/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5032 - accuracy: 0.7600 - val_loss: 0.2786 - val_accuracy: 0.9630\n",
            "Epoch 574/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5008 - accuracy: 0.7575 - val_loss: 0.2440 - val_accuracy: 0.9630\n",
            "Epoch 575/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5021 - accuracy: 0.7575 - val_loss: 0.2603 - val_accuracy: 0.9630\n",
            "Epoch 576/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5044 - accuracy: 0.7625 - val_loss: 0.2583 - val_accuracy: 0.9630\n",
            "Epoch 577/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5030 - accuracy: 0.7775 - val_loss: 0.2552 - val_accuracy: 0.9630\n",
            "Epoch 578/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5026 - accuracy: 0.7650 - val_loss: 0.2670 - val_accuracy: 0.9630\n",
            "Epoch 579/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5050 - accuracy: 0.7650 - val_loss: 0.2544 - val_accuracy: 0.9630\n",
            "Epoch 580/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5021 - accuracy: 0.7650 - val_loss: 0.2624 - val_accuracy: 0.9630\n",
            "Epoch 581/800\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.5026 - accuracy: 0.7650 - val_loss: 0.2458 - val_accuracy: 0.9630\n",
            "Epoch 582/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5016 - accuracy: 0.7600 - val_loss: 0.2594 - val_accuracy: 0.9630\n",
            "Epoch 583/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5013 - accuracy: 0.7675 - val_loss: 0.2554 - val_accuracy: 0.9630\n",
            "Epoch 584/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5025 - accuracy: 0.7675 - val_loss: 0.2574 - val_accuracy: 0.9630\n",
            "Epoch 585/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5016 - accuracy: 0.7650 - val_loss: 0.2407 - val_accuracy: 0.9630\n",
            "Epoch 586/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5019 - accuracy: 0.7750 - val_loss: 0.2491 - val_accuracy: 0.9630\n",
            "Epoch 587/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5011 - accuracy: 0.7650 - val_loss: 0.2600 - val_accuracy: 0.9630\n",
            "Epoch 588/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5020 - accuracy: 0.7650 - val_loss: 0.2478 - val_accuracy: 0.9630\n",
            "Epoch 589/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5028 - accuracy: 0.7625 - val_loss: 0.2593 - val_accuracy: 0.9630\n",
            "Epoch 590/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5017 - accuracy: 0.7700 - val_loss: 0.2598 - val_accuracy: 0.9630\n",
            "Epoch 591/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5040 - accuracy: 0.7650 - val_loss: 0.2407 - val_accuracy: 0.9630\n",
            "Epoch 592/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5038 - accuracy: 0.7650 - val_loss: 0.2750 - val_accuracy: 0.9630\n",
            "Epoch 593/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5023 - accuracy: 0.7675 - val_loss: 0.2405 - val_accuracy: 0.9630\n",
            "Epoch 594/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5027 - accuracy: 0.7575 - val_loss: 0.2689 - val_accuracy: 0.9630\n",
            "Epoch 595/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5011 - accuracy: 0.7675 - val_loss: 0.2441 - val_accuracy: 0.9630\n",
            "Epoch 596/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5005 - accuracy: 0.7625 - val_loss: 0.2528 - val_accuracy: 0.9630\n",
            "Epoch 597/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.7700 - val_loss: 0.2557 - val_accuracy: 0.9630\n",
            "Epoch 598/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5017 - accuracy: 0.7675 - val_loss: 0.2485 - val_accuracy: 0.9630\n",
            "Epoch 599/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5001 - accuracy: 0.7650 - val_loss: 0.2578 - val_accuracy: 0.9630\n",
            "Epoch 600/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5007 - accuracy: 0.7600 - val_loss: 0.2497 - val_accuracy: 0.9630\n",
            "Epoch 601/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5025 - accuracy: 0.7650 - val_loss: 0.2552 - val_accuracy: 0.9630\n",
            "Epoch 602/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5010 - accuracy: 0.7600 - val_loss: 0.2546 - val_accuracy: 0.9630\n",
            "Epoch 603/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5021 - accuracy: 0.7700 - val_loss: 0.2573 - val_accuracy: 0.9630\n",
            "Epoch 604/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7575 - val_loss: 0.2481 - val_accuracy: 0.9630\n",
            "Epoch 605/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5026 - accuracy: 0.7600 - val_loss: 0.2500 - val_accuracy: 0.9630\n",
            "Epoch 606/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5006 - accuracy: 0.7625 - val_loss: 0.2456 - val_accuracy: 0.9630\n",
            "Epoch 607/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5010 - accuracy: 0.7600 - val_loss: 0.2450 - val_accuracy: 0.9630\n",
            "Epoch 608/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7625 - val_loss: 0.2517 - val_accuracy: 0.9630\n",
            "Epoch 609/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5010 - accuracy: 0.7650 - val_loss: 0.2571 - val_accuracy: 0.9630\n",
            "Epoch 610/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7675 - val_loss: 0.2408 - val_accuracy: 0.9630\n",
            "Epoch 611/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.7775 - val_loss: 0.2471 - val_accuracy: 0.9630\n",
            "Epoch 612/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5028 - accuracy: 0.7600 - val_loss: 0.2552 - val_accuracy: 0.9630\n",
            "Epoch 613/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5049 - accuracy: 0.7575 - val_loss: 0.2416 - val_accuracy: 0.9630\n",
            "Epoch 614/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5036 - accuracy: 0.7650 - val_loss: 0.2596 - val_accuracy: 0.9630\n",
            "Epoch 615/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5002 - accuracy: 0.7675 - val_loss: 0.2348 - val_accuracy: 0.9630\n",
            "Epoch 616/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.7675 - val_loss: 0.2543 - val_accuracy: 0.9630\n",
            "Epoch 617/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7650 - val_loss: 0.2482 - val_accuracy: 0.9630\n",
            "Epoch 618/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5025 - accuracy: 0.7650 - val_loss: 0.2486 - val_accuracy: 0.9630\n",
            "Epoch 619/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5002 - accuracy: 0.7675 - val_loss: 0.2371 - val_accuracy: 0.9630\n",
            "Epoch 620/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5037 - accuracy: 0.7700 - val_loss: 0.2529 - val_accuracy: 0.9630\n",
            "Epoch 621/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5067 - accuracy: 0.7575 - val_loss: 0.2563 - val_accuracy: 0.9630\n",
            "Epoch 622/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.7650 - val_loss: 0.2374 - val_accuracy: 0.9630\n",
            "Epoch 623/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4995 - accuracy: 0.7725 - val_loss: 0.2502 - val_accuracy: 0.9630\n",
            "Epoch 624/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5006 - accuracy: 0.7650 - val_loss: 0.2619 - val_accuracy: 0.9630\n",
            "Epoch 625/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4987 - accuracy: 0.7725 - val_loss: 0.2300 - val_accuracy: 0.9630\n",
            "Epoch 626/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5010 - accuracy: 0.7775 - val_loss: 0.2485 - val_accuracy: 0.9630\n",
            "Epoch 627/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.7775 - val_loss: 0.2538 - val_accuracy: 0.9630\n",
            "Epoch 628/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4994 - accuracy: 0.7650 - val_loss: 0.2440 - val_accuracy: 0.9630\n",
            "Epoch 629/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5000 - accuracy: 0.7625 - val_loss: 0.2490 - val_accuracy: 0.9630\n",
            "Epoch 630/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5002 - accuracy: 0.7650 - val_loss: 0.2318 - val_accuracy: 0.9630\n",
            "Epoch 631/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5013 - accuracy: 0.7600 - val_loss: 0.2560 - val_accuracy: 0.9630\n",
            "Epoch 632/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4997 - accuracy: 0.7650 - val_loss: 0.2411 - val_accuracy: 0.9630\n",
            "Epoch 633/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7650 - val_loss: 0.2500 - val_accuracy: 0.9630\n",
            "Epoch 634/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5008 - accuracy: 0.7600 - val_loss: 0.2520 - val_accuracy: 0.9630\n",
            "Epoch 635/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4998 - accuracy: 0.7625 - val_loss: 0.2415 - val_accuracy: 0.9630\n",
            "Epoch 636/800\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4993 - accuracy: 0.7675 - val_loss: 0.2468 - val_accuracy: 0.9630\n",
            "Epoch 637/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7700 - val_loss: 0.2487 - val_accuracy: 0.9630\n",
            "Epoch 638/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4983 - accuracy: 0.7725 - val_loss: 0.2493 - val_accuracy: 0.9630\n",
            "Epoch 639/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4990 - accuracy: 0.7675 - val_loss: 0.2356 - val_accuracy: 0.9630\n",
            "Epoch 640/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4985 - accuracy: 0.7650 - val_loss: 0.2497 - val_accuracy: 0.9630\n",
            "Epoch 641/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4996 - accuracy: 0.7700 - val_loss: 0.2390 - val_accuracy: 0.9630\n",
            "Epoch 642/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4992 - accuracy: 0.7600 - val_loss: 0.2439 - val_accuracy: 0.9630\n",
            "Epoch 643/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7625 - val_loss: 0.2445 - val_accuracy: 0.9630\n",
            "Epoch 644/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5006 - accuracy: 0.7675 - val_loss: 0.2442 - val_accuracy: 0.9630\n",
            "Epoch 645/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4985 - accuracy: 0.7600 - val_loss: 0.2337 - val_accuracy: 0.9630\n",
            "Epoch 646/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4992 - accuracy: 0.7675 - val_loss: 0.2479 - val_accuracy: 0.9630\n",
            "Epoch 647/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5035 - accuracy: 0.7775 - val_loss: 0.2364 - val_accuracy: 0.9630\n",
            "Epoch 648/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5057 - accuracy: 0.7675 - val_loss: 0.2525 - val_accuracy: 0.9630\n",
            "Epoch 649/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4974 - accuracy: 0.7600 - val_loss: 0.2261 - val_accuracy: 0.9630\n",
            "Epoch 650/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5018 - accuracy: 0.7800 - val_loss: 0.2578 - val_accuracy: 0.9630\n",
            "Epoch 651/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4984 - accuracy: 0.7650 - val_loss: 0.2306 - val_accuracy: 0.9630\n",
            "Epoch 652/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.7700 - val_loss: 0.2409 - val_accuracy: 0.9630\n",
            "Epoch 653/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4995 - accuracy: 0.7775 - val_loss: 0.2474 - val_accuracy: 0.9630\n",
            "Epoch 654/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4988 - accuracy: 0.7700 - val_loss: 0.2446 - val_accuracy: 0.9630\n",
            "Epoch 655/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5003 - accuracy: 0.7725 - val_loss: 0.2360 - val_accuracy: 0.9630\n",
            "Epoch 656/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5018 - accuracy: 0.7650 - val_loss: 0.2331 - val_accuracy: 0.9630\n",
            "Epoch 657/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4964 - accuracy: 0.7725 - val_loss: 0.2547 - val_accuracy: 0.9630\n",
            "Epoch 658/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5016 - accuracy: 0.7625 - val_loss: 0.2394 - val_accuracy: 0.9630\n",
            "Epoch 659/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7750 - val_loss: 0.2432 - val_accuracy: 0.9630\n",
            "Epoch 660/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4992 - accuracy: 0.7850 - val_loss: 0.2310 - val_accuracy: 0.9630\n",
            "Epoch 661/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5005 - accuracy: 0.7675 - val_loss: 0.2491 - val_accuracy: 0.9630\n",
            "Epoch 662/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4985 - accuracy: 0.7675 - val_loss: 0.2316 - val_accuracy: 0.9630\n",
            "Epoch 663/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4982 - accuracy: 0.7675 - val_loss: 0.2428 - val_accuracy: 0.9630\n",
            "Epoch 664/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4982 - accuracy: 0.7775 - val_loss: 0.2417 - val_accuracy: 0.9630\n",
            "Epoch 665/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4978 - accuracy: 0.7625 - val_loss: 0.2337 - val_accuracy: 0.9630\n",
            "Epoch 666/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4983 - accuracy: 0.7575 - val_loss: 0.2346 - val_accuracy: 0.9630\n",
            "Epoch 667/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4977 - accuracy: 0.7575 - val_loss: 0.2433 - val_accuracy: 0.9630\n",
            "Epoch 668/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4994 - accuracy: 0.7725 - val_loss: 0.2501 - val_accuracy: 0.9630\n",
            "Epoch 669/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4997 - accuracy: 0.7675 - val_loss: 0.2362 - val_accuracy: 0.9630\n",
            "Epoch 670/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4994 - accuracy: 0.7775 - val_loss: 0.2414 - val_accuracy: 0.9630\n",
            "Epoch 671/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5027 - accuracy: 0.7725 - val_loss: 0.2287 - val_accuracy: 0.9630\n",
            "Epoch 672/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.7625 - val_loss: 0.2577 - val_accuracy: 0.9630\n",
            "Epoch 673/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5029 - accuracy: 0.7775 - val_loss: 0.2278 - val_accuracy: 0.9630\n",
            "Epoch 674/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5018 - accuracy: 0.7700 - val_loss: 0.2485 - val_accuracy: 0.9630\n",
            "Epoch 675/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7700 - val_loss: 0.2302 - val_accuracy: 0.9630\n",
            "Epoch 676/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4986 - accuracy: 0.7725 - val_loss: 0.2525 - val_accuracy: 0.9630\n",
            "Epoch 677/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4965 - accuracy: 0.7775 - val_loss: 0.2308 - val_accuracy: 0.9630\n",
            "Epoch 678/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7625 - val_loss: 0.2332 - val_accuracy: 0.9630\n",
            "Epoch 679/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.7700 - val_loss: 0.2425 - val_accuracy: 0.9630\n",
            "Epoch 680/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5011 - accuracy: 0.7750 - val_loss: 0.2489 - val_accuracy: 0.9630\n",
            "Epoch 681/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.7775 - val_loss: 0.2213 - val_accuracy: 0.9630\n",
            "Epoch 682/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7725 - val_loss: 0.2450 - val_accuracy: 0.9630\n",
            "Epoch 683/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4988 - accuracy: 0.7700 - val_loss: 0.2354 - val_accuracy: 0.9630\n",
            "Epoch 684/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4975 - accuracy: 0.7625 - val_loss: 0.2291 - val_accuracy: 0.9630\n",
            "Epoch 685/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7675 - val_loss: 0.2489 - val_accuracy: 0.9630\n",
            "Epoch 686/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4980 - accuracy: 0.7775 - val_loss: 0.2408 - val_accuracy: 0.9630\n",
            "Epoch 687/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4988 - accuracy: 0.7625 - val_loss: 0.2414 - val_accuracy: 0.9630\n",
            "Epoch 688/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4970 - accuracy: 0.7750 - val_loss: 0.2416 - val_accuracy: 0.9630\n",
            "Epoch 689/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4974 - accuracy: 0.7775 - val_loss: 0.2335 - val_accuracy: 0.9630\n",
            "Epoch 690/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4980 - accuracy: 0.7575 - val_loss: 0.2325 - val_accuracy: 0.9630\n",
            "Epoch 691/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4970 - accuracy: 0.7700 - val_loss: 0.2378 - val_accuracy: 0.9630\n",
            "Epoch 692/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5001 - accuracy: 0.7725 - val_loss: 0.2402 - val_accuracy: 0.9630\n",
            "Epoch 693/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4974 - accuracy: 0.7700 - val_loss: 0.2249 - val_accuracy: 0.9630\n",
            "Epoch 694/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4987 - accuracy: 0.7675 - val_loss: 0.2436 - val_accuracy: 0.9630\n",
            "Epoch 695/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4978 - accuracy: 0.7725 - val_loss: 0.2283 - val_accuracy: 0.9630\n",
            "Epoch 696/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.7750 - val_loss: 0.2414 - val_accuracy: 0.9630\n",
            "Epoch 697/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4973 - accuracy: 0.7750 - val_loss: 0.2354 - val_accuracy: 0.9630\n",
            "Epoch 698/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7675 - val_loss: 0.2359 - val_accuracy: 0.9630\n",
            "Epoch 699/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.7675 - val_loss: 0.2402 - val_accuracy: 0.9630\n",
            "Epoch 700/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5001 - accuracy: 0.7750 - val_loss: 0.2215 - val_accuracy: 0.9630\n",
            "Epoch 701/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4957 - accuracy: 0.7750 - val_loss: 0.2525 - val_accuracy: 0.9630\n",
            "Epoch 702/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 0.7725 - val_loss: 0.2379 - val_accuracy: 0.9630\n",
            "Epoch 703/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4975 - accuracy: 0.7700 - val_loss: 0.2398 - val_accuracy: 0.9630\n",
            "Epoch 704/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4983 - accuracy: 0.7750 - val_loss: 0.2363 - val_accuracy: 0.9630\n",
            "Epoch 705/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.7600 - val_loss: 0.2267 - val_accuracy: 0.9630\n",
            "Epoch 706/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5033 - accuracy: 0.7525 - val_loss: 0.2393 - val_accuracy: 0.9630\n",
            "Epoch 707/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4967 - accuracy: 0.7700 - val_loss: 0.2327 - val_accuracy: 0.9630\n",
            "Epoch 708/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4988 - accuracy: 0.7625 - val_loss: 0.2309 - val_accuracy: 0.9630\n",
            "Epoch 709/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7775 - val_loss: 0.2436 - val_accuracy: 0.9630\n",
            "Epoch 710/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.7725 - val_loss: 0.2309 - val_accuracy: 0.9630\n",
            "Epoch 711/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4992 - accuracy: 0.7725 - val_loss: 0.2236 - val_accuracy: 0.9630\n",
            "Epoch 712/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.7675 - val_loss: 0.2490 - val_accuracy: 0.9630\n",
            "Epoch 713/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4980 - accuracy: 0.7750 - val_loss: 0.2233 - val_accuracy: 0.9630\n",
            "Epoch 714/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7700 - val_loss: 0.2424 - val_accuracy: 0.9630\n",
            "Epoch 715/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.7750 - val_loss: 0.2306 - val_accuracy: 0.9630\n",
            "Epoch 716/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7775 - val_loss: 0.2279 - val_accuracy: 0.9630\n",
            "Epoch 717/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7675 - val_loss: 0.2439 - val_accuracy: 0.9630\n",
            "Epoch 718/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4988 - accuracy: 0.7725 - val_loss: 0.2218 - val_accuracy: 0.9630\n",
            "Epoch 719/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5008 - accuracy: 0.7625 - val_loss: 0.2403 - val_accuracy: 0.9630\n",
            "Epoch 720/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4965 - accuracy: 0.7750 - val_loss: 0.2310 - val_accuracy: 0.9630\n",
            "Epoch 721/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4986 - accuracy: 0.7725 - val_loss: 0.2270 - val_accuracy: 0.9630\n",
            "Epoch 722/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4975 - accuracy: 0.7675 - val_loss: 0.2370 - val_accuracy: 0.9630\n",
            "Epoch 723/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4969 - accuracy: 0.7675 - val_loss: 0.2340 - val_accuracy: 0.9630\n",
            "Epoch 724/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4968 - accuracy: 0.7650 - val_loss: 0.2323 - val_accuracy: 0.9630\n",
            "Epoch 725/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7750 - val_loss: 0.2300 - val_accuracy: 0.9630\n",
            "Epoch 726/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4991 - accuracy: 0.7725 - val_loss: 0.2408 - val_accuracy: 0.9630\n",
            "Epoch 727/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4959 - accuracy: 0.7575 - val_loss: 0.2222 - val_accuracy: 0.9630\n",
            "Epoch 728/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4981 - accuracy: 0.7675 - val_loss: 0.2383 - val_accuracy: 0.9630\n",
            "Epoch 729/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4957 - accuracy: 0.7775 - val_loss: 0.2308 - val_accuracy: 0.9630\n",
            "Epoch 730/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4962 - accuracy: 0.7750 - val_loss: 0.2360 - val_accuracy: 0.9630\n",
            "Epoch 731/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4958 - accuracy: 0.7775 - val_loss: 0.2283 - val_accuracy: 0.9630\n",
            "Epoch 732/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4986 - accuracy: 0.7600 - val_loss: 0.2276 - val_accuracy: 0.9630\n",
            "Epoch 733/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4960 - accuracy: 0.7800 - val_loss: 0.2472 - val_accuracy: 0.9630\n",
            "Epoch 734/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4977 - accuracy: 0.7775 - val_loss: 0.2255 - val_accuracy: 0.9630\n",
            "Epoch 735/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7700 - val_loss: 0.2331 - val_accuracy: 0.9630\n",
            "Epoch 736/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5020 - accuracy: 0.7725 - val_loss: 0.2387 - val_accuracy: 0.9630\n",
            "Epoch 737/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4959 - accuracy: 0.7800 - val_loss: 0.2270 - val_accuracy: 0.9630\n",
            "Epoch 738/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7775 - val_loss: 0.2279 - val_accuracy: 0.9630\n",
            "Epoch 739/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4949 - accuracy: 0.7750 - val_loss: 0.2445 - val_accuracy: 0.9630\n",
            "Epoch 740/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7625 - val_loss: 0.2340 - val_accuracy: 0.9630\n",
            "Epoch 741/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4973 - accuracy: 0.7675 - val_loss: 0.2351 - val_accuracy: 0.9630\n",
            "Epoch 742/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4965 - accuracy: 0.7800 - val_loss: 0.2406 - val_accuracy: 0.9630\n",
            "Epoch 743/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4964 - accuracy: 0.7750 - val_loss: 0.2285 - val_accuracy: 0.9630\n",
            "Epoch 744/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4961 - accuracy: 0.7675 - val_loss: 0.2299 - val_accuracy: 0.9630\n",
            "Epoch 745/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4974 - accuracy: 0.7750 - val_loss: 0.2312 - val_accuracy: 0.9630\n",
            "Epoch 746/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4954 - accuracy: 0.7775 - val_loss: 0.2351 - val_accuracy: 0.9630\n",
            "Epoch 747/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4983 - accuracy: 0.7650 - val_loss: 0.2303 - val_accuracy: 0.9630\n",
            "Epoch 748/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4977 - accuracy: 0.7725 - val_loss: 0.2354 - val_accuracy: 0.9630\n",
            "Epoch 749/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4953 - accuracy: 0.7750 - val_loss: 0.2309 - val_accuracy: 0.9630\n",
            "Epoch 750/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4979 - accuracy: 0.7625 - val_loss: 0.2290 - val_accuracy: 0.9630\n",
            "Epoch 751/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4955 - accuracy: 0.7775 - val_loss: 0.2427 - val_accuracy: 0.9630\n",
            "Epoch 752/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4958 - accuracy: 0.7800 - val_loss: 0.2285 - val_accuracy: 0.9630\n",
            "Epoch 753/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4960 - accuracy: 0.7675 - val_loss: 0.2336 - val_accuracy: 0.9630\n",
            "Epoch 754/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.7750 - val_loss: 0.2375 - val_accuracy: 0.9630\n",
            "Epoch 755/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4968 - accuracy: 0.7750 - val_loss: 0.2271 - val_accuracy: 0.9630\n",
            "Epoch 756/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4962 - accuracy: 0.7700 - val_loss: 0.2295 - val_accuracy: 0.9630\n",
            "Epoch 757/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5022 - accuracy: 0.7700 - val_loss: 0.2333 - val_accuracy: 0.9630\n",
            "Epoch 758/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4995 - accuracy: 0.7775 - val_loss: 0.2324 - val_accuracy: 0.9630\n",
            "Epoch 759/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7750 - val_loss: 0.2253 - val_accuracy: 0.9630\n",
            "Epoch 760/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7650 - val_loss: 0.2322 - val_accuracy: 0.9630\n",
            "Epoch 761/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4959 - accuracy: 0.7800 - val_loss: 0.2347 - val_accuracy: 0.9630\n",
            "Epoch 762/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4969 - accuracy: 0.7700 - val_loss: 0.2297 - val_accuracy: 0.9630\n",
            "Epoch 763/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.7775 - val_loss: 0.2366 - val_accuracy: 0.9630\n",
            "Epoch 764/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.7600 - val_loss: 0.2267 - val_accuracy: 0.9630\n",
            "Epoch 765/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4955 - accuracy: 0.7650 - val_loss: 0.2346 - val_accuracy: 0.9630\n",
            "Epoch 766/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4966 - accuracy: 0.7775 - val_loss: 0.2348 - val_accuracy: 0.9630\n",
            "Epoch 767/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4976 - accuracy: 0.7700 - val_loss: 0.2292 - val_accuracy: 0.9630\n",
            "Epoch 768/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4971 - accuracy: 0.7700 - val_loss: 0.2378 - val_accuracy: 0.9630\n",
            "Epoch 769/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4956 - accuracy: 0.7775 - val_loss: 0.2308 - val_accuracy: 0.9630\n",
            "Epoch 770/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4957 - accuracy: 0.7750 - val_loss: 0.2308 - val_accuracy: 0.9630\n",
            "Epoch 771/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4952 - accuracy: 0.7775 - val_loss: 0.2280 - val_accuracy: 0.9630\n",
            "Epoch 772/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4961 - accuracy: 0.7725 - val_loss: 0.2325 - val_accuracy: 0.9630\n",
            "Epoch 773/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4960 - accuracy: 0.7650 - val_loss: 0.2266 - val_accuracy: 0.9630\n",
            "Epoch 774/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4971 - accuracy: 0.7750 - val_loss: 0.2279 - val_accuracy: 0.9630\n",
            "Epoch 775/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4958 - accuracy: 0.7750 - val_loss: 0.2251 - val_accuracy: 0.9630\n",
            "Epoch 776/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4969 - accuracy: 0.7775 - val_loss: 0.2273 - val_accuracy: 0.9630\n",
            "Epoch 777/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7775 - val_loss: 0.2192 - val_accuracy: 0.9630\n",
            "Epoch 778/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4952 - accuracy: 0.7725 - val_loss: 0.2345 - val_accuracy: 0.9630\n",
            "Epoch 779/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4964 - accuracy: 0.7725 - val_loss: 0.2318 - val_accuracy: 0.9630\n",
            "Epoch 780/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4961 - accuracy: 0.7750 - val_loss: 0.2315 - val_accuracy: 0.9630\n",
            "Epoch 781/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4961 - accuracy: 0.7725 - val_loss: 0.2256 - val_accuracy: 0.9630\n",
            "Epoch 782/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7650 - val_loss: 0.2227 - val_accuracy: 0.9630\n",
            "Epoch 783/800\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4950 - accuracy: 0.7750 - val_loss: 0.2332 - val_accuracy: 0.9630\n",
            "Epoch 784/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4974 - accuracy: 0.7800 - val_loss: 0.2289 - val_accuracy: 0.9630\n",
            "Epoch 785/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7725 - val_loss: 0.2247 - val_accuracy: 0.9630\n",
            "Epoch 786/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.7750 - val_loss: 0.2289 - val_accuracy: 0.9630\n",
            "Epoch 787/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4950 - accuracy: 0.7750 - val_loss: 0.2310 - val_accuracy: 0.9630\n",
            "Epoch 788/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4954 - accuracy: 0.7700 - val_loss: 0.2244 - val_accuracy: 0.9630\n",
            "Epoch 789/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4998 - accuracy: 0.7725 - val_loss: 0.2292 - val_accuracy: 0.9630\n",
            "Epoch 790/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4976 - accuracy: 0.7750 - val_loss: 0.2164 - val_accuracy: 0.9630\n",
            "Epoch 791/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4975 - accuracy: 0.7700 - val_loss: 0.2341 - val_accuracy: 0.9630\n",
            "Epoch 792/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.7675 - val_loss: 0.2282 - val_accuracy: 0.9630\n",
            "Epoch 793/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4958 - accuracy: 0.7725 - val_loss: 0.2363 - val_accuracy: 0.9630\n",
            "Epoch 794/800\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4948 - accuracy: 0.7750 - val_loss: 0.2240 - val_accuracy: 0.9630\n",
            "Epoch 795/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4957 - accuracy: 0.7675 - val_loss: 0.2263 - val_accuracy: 0.9630\n",
            "Epoch 796/800\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4952 - accuracy: 0.7750 - val_loss: 0.2306 - val_accuracy: 0.9630\n",
            "Epoch 797/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4973 - accuracy: 0.7600 - val_loss: 0.2281 - val_accuracy: 0.9630\n",
            "Epoch 798/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4947 - accuracy: 0.7825 - val_loss: 0.2371 - val_accuracy: 0.9630\n",
            "Epoch 799/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4954 - accuracy: 0.7800 - val_loss: 0.2276 - val_accuracy: 0.9630\n",
            "Epoch 800/800\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5015 - accuracy: 0.7700 - val_loss: 0.2163 - val_accuracy: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a0 = model.predict(a)\n",
        "b0 = model.predict(b)\n",
        "c0 = model.predict(c)\n",
        "d0 = model.predict(d)\n",
        "e0 = model.predict(e)\n",
        "f0 = model.predict(f)\n",
        "g0 = model.predict(g)\n",
        "h0 = model.predict(h)\n",
        "i0 = model.predict(i)\n",
        "k0 = model.predict(k)\n",
        "j0 = model.predict(j)\n",
        "l0 = model.predict(l)\n",
        "print(\"elden ring's winning award(s) probability: \",a0[0][1]) #elden ring\n",
        "print(\"stray's winning award(s) probability: \",b0[0][1]) #stray\n",
        "print(\"god of war's winning award(s) probability: \",c0[0][1]) #gow\n",
        "print(\"one piece grand cruise's winning award(s) probability: \",d0[0][1]) #one piece grand cruise\n",
        "print(\"martha is dead's winning award(s) probability: \",e0[0][1]) #martha is dead\n",
        "print(\"horizon's winning award(s) probability: \",f0[0][1]) #martha is dead\n",
        "print(\"tunic's winning award(s) probability: \",g0[0][1]) #martha is dead\n",
        "print(\"a plague tale's winning award(s) probability: \",h0[0][1]) #martha is dead\n",
        "print(\"xenoblade 3's winning award(s) probability: \",i0[0][1]) #martha is dead\n",
        "print(\"sonic frontier's winning award(s) probability: \",k0[0][1]) #martha is dead\n",
        "print(\"genshit's winning award(s) probability: \",j0[0][1]) #martha is dead\n",
        "print(\"bayonetta's winning award(s) probability: \",l0[0][1]) #martha is dead\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7OKMVR7lo6f",
        "outputId": "7dbf30d9-2800-41f1-eef2-5759ace147b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "elden ring's winning award(s) probability:  0.9396236\n",
            "stray's winning award(s) probability:  0.7908871\n",
            "god of war's winning award(s) probability:  0.92575586\n",
            "one piece grand cruise's winning award(s) probability:  0.04541865\n",
            "martha is dead's winning award(s) probability:  0.04541865\n",
            "horizon's winning award(s) probability:  0.72442335\n",
            "tunic's winning award(s) probability:  0.8938426\n",
            "a plague tale's winning award(s) probability:  0.6259278\n",
            "xenoblade 3's winning award(s) probability:  0.770416\n",
            "sonic frontier's winning award(s) probability:  0.3925721\n",
            "genshit's winning award(s) probability:  0.6800668\n",
            "bayonetta's winning award(s) probability:  0.5353737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z1 = a.dot(W[-2].T)\n",
        "z2 = b.dot(W[-2].T)\n",
        "z3 = c.dot(W[-2].T)\n",
        "z4 = d.dot(W[-2].T)\n",
        "z5 = X_train[1].reshape(1,6).dot(W[-2].T)\n",
        "print('elden ring win rate:',sigmoid(z1))\n",
        "print('stray win rate: ', sigmoid(z2))\n",
        "print('god of war win rate: ', sigmoid(z3))\n",
        "print('one piece win rate:', sigmoid(z4))\n",
        "print(sigmoid(z5))\n",
        "print(validate(X_val,Y_val,W[-1]))\n"
      ],
      "metadata": {
        "id": "YW64hGupHYTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3bede074-64f8-48d6-d1a1-722518af7404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-bb919ab3c04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mz4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('elden ring: ',mlp_predict(a, W1[-1], W2[-1], W3[-1]))\n",
        "print('stray: ', mlp_predict(b, W1[-1], W2[-1], W3[-1]))\n",
        "print('gow: ',mlp_predict(c, W1[-1], W2[-1], W3[-1]))\n",
        "print('1 piece: ',mlp_predict(d, W1[-1], W2[-1], W3[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "275Nb_IJI4NS",
        "outputId": "87807c8e-456f-4a15-e85a-cb87e454fe5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elden ring:  [[0.42894171]]\n",
            "stray:  [[0.42939901]]\n",
            "gow:  [[0.42910478]]\n",
            "1 piece:  [[0.4309954]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse model\n",
        "def convertProbToLabel(predicted):\n",
        "  mask = np.array(predicted) > 0.6\n",
        "  return np.array(mask, dtype='int')\n",
        "\n",
        "def truePos(pred, label):\n",
        "  return np.multiply(pred, label)\n",
        "\n",
        "def trueNeg(pred, label):\n",
        "  return np.multiply(1 - pred, 1 - label)\n",
        "\n",
        "def falsePos(pred, label):\n",
        "  return np.multiply(1 - pred, label)\n",
        "\n",
        "def falseNeg(pred, label):\n",
        "  return np.multiply(pred, 1 - label)\n",
        "\n",
        "def sortResult(predicted_array):\n",
        "  indices = [i for i in range(data.shape[0])]\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(i+1, data.shape[0]):\n",
        "      if predicted_array[i] > predicted_array[j]:\n",
        "        temp = predicted_array[i]\n",
        "        predicted_array[i] = predicted_array[j]\n",
        "        predicted_array[j] = temp\n",
        "        temp1 = indices[i]\n",
        "        indices[i] = indices[j]\n",
        "        indices[j] = temp1\n",
        "  return indices\n",
        "\n",
        "def sortResultTest(predicted_array):\n",
        "  indices = [i for i in range(dataTest.shape[0])]\n",
        "  for i in range(dataTest.shape[0]):\n",
        "    for j in range(i+1, dataTest.shape[0]):\n",
        "      if predicted_array[i] > predicted_array[j]:\n",
        "        temp = predicted_array[i]\n",
        "        predicted_array[i] = predicted_array[j]\n",
        "        predicted_array[j] = temp\n",
        "        temp1 = indices[i]\n",
        "        indices[i] = indices[j]\n",
        "        indices[j] = temp1\n",
        "  return indices"
      ],
      "metadata": {
        "id": "G5u-_51crUx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "dataTest, labelTest = processData('TestData.csv')\n",
        "\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('myModel_softmax_2_5.h5')\n",
        "\n",
        "test_predicted = []\n",
        "for i in range(dataTest.shape[0]):\n",
        "  item = dataTest[i].reshape(1,6)\n",
        "  test_predicted.append(model.predict(item)[0][1])\n",
        "\n",
        "label0 = labelTest\n",
        "predictedLabel = convertProbToLabel(test_predicted)\n",
        "result = bitwise_xor(predictedLabel, label0)\n",
        "print('accuracy', np.sum(result) / result.shape[0])\n",
        "print('true positive', np.sum(truePos(predictedLabel, label0)))\n",
        "print('true negative', np.sum(trueNeg(predictedLabel, label0)))\n",
        "print('false positive', np.sum(falsePos(predictedLabel, label0)))\n",
        "print('false negative', np.sum(falseNeg(predictedLabel, label0)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "predictedCopy = test_predicted.copy()\n",
        "labelCopy = labelTest.copy()\n",
        "predictedLabelCopy = predictedLabel.copy()\n",
        "indices= np.array(sortResultTest(predictedCopy), dtype='int')\n",
        "indices = indices.astype(int)\n",
        "\n",
        "#predictedCopy.sort()\n",
        "#labelCopy.sort()\n",
        "#predictedLabelCopy.sort()\n",
        "out = []\n",
        "for i in indices:\n",
        "  out.append(labelCopy[i])\n",
        "\n",
        "predictedLabel = convertProbToLabel(predictedCopy)\n",
        "\n",
        "plt.plot(predictedCopy, label='Predicted Probability')\n",
        "plt.plot(predictedLabel, label='Predicted Label')\n",
        "plt.plot(out,'r.', label='True Label')\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Z8QukS6-tQ0",
        "outputId": "3d1d9e67-4e13-4acb-d0a6-3862eb94d856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0002703690362379912\n",
            "0.0003755163349605708\n",
            "0.013028390051350877\n",
            "1.0\n",
            "0.00019638648860958367\n",
            "0.040475114406879495\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0010363367265604543\n",
            "0.0011300692391909845\n",
            "1.0\n",
            "0.00024113817217265494\n",
            "0.03181336623076253\n",
            "0.027787274657720535\n",
            "0.032983512505183415\n",
            "0.013168572598318478\n",
            "0.0015554431667121101\n",
            "1.0\n",
            "0.000672270634647671\n",
            "0.0796392413580539\n",
            "1.0\n",
            "0.009523809523809525\n",
            "0.006712398735468425\n",
            "0.0071635632065390874\n",
            "0.003249094021174919\n",
            "0.05362602349814658\n",
            "0.018644405927778527\n",
            "0.0025348542458808617\n",
            "0.009052527816430414\n",
            "1.983542126275617e-05\n",
            "1.0\n",
            "0.11978243827074593\n",
            "0.009068930198116174\n",
            "0.01184965438768266\n",
            "1.0\n",
            "0.006600660066006601\n",
            "0.014491209460137806\n",
            "1.0\n",
            "0.014084507042253521\n",
            "0.05\n",
            "1.0\n",
            "0.02075870769047956\n",
            "1.0\n",
            "1.0\n",
            "0.04647210602067426\n",
            "0.0017777777777777779\n",
            "1.0\n",
            "0.03736426478576777\n",
            "0.034596123346632736\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "accuracy 0.8\n",
            "true positive 12.0\n",
            "true negative 28.0\n",
            "false positive 8.0\n",
            "false negative 2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3fd9cf73a0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting data\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('myModel_softmax_2_5.h5')\n",
        "predicted = []\n",
        "for i in range(data.shape[0]):\n",
        "  item = data[i].reshape(1,6)\n",
        "  predicted.append(model.predict(item)[0][1])\n",
        "\n",
        "print(predicted[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z7AmPB3mBX1",
        "outputId": "b096be18-508f-47fc-bcbf-8c456f7530d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[0.79694873, 0.91018766, 0.54264826, 0.7660821, 0.8302754, 0.7576742, 0.6608934, 0.805579, 0.8612978, 0.9544305]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label0 = label\n",
        "predictedLabel = convertProbToLabel(predicted)\n",
        "result = bitwise_xor(predictedLabel, label0)\n",
        "print('accuracy', np.sum(result) / result.shape[0])\n",
        "print('true positive', np.sum(truePos(predictedLabel, label0)))\n",
        "print('true negative', np.sum(trueNeg(predictedLabel, label0)))\n",
        "print('false positive', np.sum(falsePos(predictedLabel, label0)))\n",
        "print('false negative', np.sum(falseNeg(predictedLabel, label0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ_IdEL2_Hj6",
        "outputId": "f927e7a0-f970-4203-fc4d-7b52f793f6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.775175644028103\n",
            "true positive 110.0\n",
            "true negative 221.0\n",
            "false positive 68.0\n",
            "false negative 28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "predictedCopy = predicted.copy()\n",
        "labelCopy = label.copy()\n",
        "predictedLabelCopy = predictedLabel.copy()\n",
        "indices= np.array(sortResult(predictedCopy), dtype='int')\n",
        "indices = indices.astype(int)\n",
        "\n",
        "#predictedCopy.sort()\n",
        "#labelCopy.sort()\n",
        "#predictedLabelCopy.sort()\n",
        "out = []\n",
        "for i in indices:\n",
        "  out.append(labelCopy[i])\n",
        "\n",
        "predictedLabel = convertProbToLabel(predictedCopy)\n",
        "\n",
        "plt.plot(predictedCopy, 'b', label='Predicted Probability')\n",
        "plt.plot(predictedLabel, label='Predicted Label')\n",
        "plt.plot(out,'r.', label='True Label')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ztRYo2TNomwc",
        "outputId": "f149a5ea-50f3-452e-ed7b-bfe43c19261f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating the dataset\n",
        "data = {'Training Accuracy':76, 'Validating Accuracy':96}\n",
        "courses = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (4, 4))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(courses, values, color ='blue',\n",
        "        width = 0.2)\n",
        "\n",
        "plt.xlabel(\"Accuracy Type\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Logistics Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "42a7NmFg_Bl0",
        "outputId": "0ba692ff-e459-4fa2-bcaa-e1198b667106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y_pred = np.array(predictedLabel)\n",
        "y_test = np.array(out)\n",
        "labels = [\"0\", \"1\"]\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "pJJKO_MhKlMM",
        "outputId": "757e059e-c521-4579-c0e8-49266ebc76da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "truePopulation = data[label > 0.5]\n",
        "falsePopulation = data[label < 0.5]\n",
        "\n",
        "print(truePopulation.shape)\n",
        "print(falsePopulation.shape)\n",
        "cov1 = np.cov(truePopulation.T)\n",
        "cov0 = np.cov(falsePopulation.T)\n",
        "print((cov0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkBg5Q3HLvLt",
        "outputId": "1cdc7138-f9c5-4068-ad3a-a6be2f8bfc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(178, 6)\n",
            "(249, 6)\n",
            "[[ 2.63641659e-02  1.65693552e-02 -1.49836515e-02  1.37713031e-02\n",
            "   1.62072013e-02 -1.91273057e-03]\n",
            " [ 1.65693552e-02  3.64923770e-02 -6.60964603e-03 -7.13180149e-03\n",
            "  -6.25701261e-03 -5.36874615e-02]\n",
            " [-1.49836515e-02 -6.60964603e-03  7.88986023e-02 -7.00540137e-02\n",
            "  -3.59265722e-02 -1.44916112e-01]\n",
            " [ 1.37713031e-02 -7.13180149e-03 -7.00540137e-02  2.50097163e-01\n",
            "   1.03575593e-01  1.49144967e-01]\n",
            " [ 1.62072013e-02 -6.25701261e-03 -3.59265722e-02  1.03575593e-01\n",
            "   2.47603316e-01  1.24287472e-01]\n",
            " [-1.91273057e-03 -5.36874615e-02 -1.44916112e-01  1.49144967e-01\n",
            "   1.24287472e-01  2.36225547e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convertNaN(A):\n",
        "  mask = np.isnan(A)\n",
        "  A[mask] = 0\n",
        "  return A\n",
        "\n",
        "def BarlettTest(variances, n, k):\n",
        "  pooled = (np.sum(np.multiply(variances, n), axis=0))/(np.sum(n) - k)\n",
        "  nominator = (np.sum(n) - k)*convertNaN(np.log(pooled)) - np.sum(np.multiply((n-1), convertNaN(np.log(variances))), axis=0)\n",
        "  denominator = 1 + (np.sum(1 / (n  - 1)) - 1/(np.sum(n) - 1))/(3*k - 3)\n",
        "  return nominator/denominator\n",
        "\n",
        "\n",
        "variances = np.array([cov0, cov1])\n",
        "n = np.array([falsePopulation.shape[0], truePopulation.shape[0]]).reshape(2,1,1)\n",
        "print(BarlettTest(variances, n, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juBOVgEYSGhT",
        "outputId": "58220490-2934-4c4c-bc66-dde36bac1874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  85.62345768  132.19357377    0.           21.13046708  102.15504948\n",
            "     0.        ]\n",
            " [ 132.19357377   22.70374765 1121.80336858 1424.61174842    0.\n",
            "     0.        ]\n",
            " [   0.         1121.80336858   14.07056523    0.            0.\n",
            "     0.        ]\n",
            " [  21.13046708 1424.61174842    0.            2.15246303    3.87849176\n",
            "    34.84670951]\n",
            " [ 102.15504948    0.            0.            3.87849176    1.99764878\n",
            "    53.9458822 ]\n",
            " [   0.            0.            0.           34.84670951   53.9458822\n",
            "     2.12195772]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-d67b613a8208>:8: RuntimeWarning: invalid value encountered in log\n",
            "  nominator = (np.sum(n) - k)*convertNaN(np.log(pooled)) - np.sum(np.multiply((n-1), convertNaN(np.log(variances))), axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "fgH6X-Q3GXQ1"
      }
    }
  ]
}